{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "view-in-github"
            },
            "source": [
                "<a href=\"https://colab.research.google.com/github/ThanosApostolou/aics-pattern-recognition/blob/main/IMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "63e0249e",
            "metadata": {
                "id": "63e0249e"
            },
            "source": [
                "<a href=\"https://colab.research.google.com/github/ThanosApostolou/aics-pattern-recognition/blob/main/IMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1oA4US2rVw3X",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "1oA4US2rVw3X",
                "outputId": "6eff896b-5c10-4ba0-bfaf-402be6c9ec0d",
                "tags": []
            },
            "outputs": [],
            "source": [
                "# INSTALL DEPENDENCIES\n",
                "# Uncomment and run only once.\n",
                "%pip install matplotlib numpy pandas scikit-learn scipy tensorflow pyclustering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4a24235d-b5ff-4fed-8231-bbaf33d6c772",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "4a24235d-b5ff-4fed-8231-bbaf33d6c772",
                "outputId": "d0e5f31a-d13a-4747-b853-acce378bf6e7",
                "tags": []
            },
            "outputs": [],
            "source": [
                "# IMPORTS AND GLOBAL CONSTANTS\n",
                "\n",
                "# Load the TensorBoard notebook extension\n",
                "%load_ext tensorboard\n",
                "\n",
                "import math\n",
                "import tensorflow as tf\n",
                "import datetime, os\n",
                "import numpy as np \n",
                "import pandas as pd\n",
                "import os\n",
                "import matplotlib.pyplot as plt\n",
                "import typing\n",
                "import numpy.typing as np_typing\n",
                "\n",
                "##MAIN PROGRAM VARIABLES##\n",
                "##(0): dataset: np array of strings\n",
                "##(1): dataframe: original dataset in its primal form\n",
                "##(2): ratings_num_df: new dataframe storing the number of rated items per unique user\n",
                "##(3): ratings_span_df: new dataframe storing the timespan in days for each user\n",
                "##(4): minimum_ratings - maximum_ratings => ratings_df=> (i) final_df\n",
                "\n",
                "# Constants\n",
                "DATASET_FILE_PATH = \"./Dataset.npy\"\n",
                "#Define the figures path\n",
                "FIGURES_PATH = \"figures\"\n",
                "os.makedirs(FIGURES_PATH, exist_ok=True)\n",
                "# #Define the data folder path\n",
                "DATAFOLDER_PATH = \"datafiles\"\n",
                "os.makedirs(DATAFOLDER_PATH, exist_ok=True)\n",
                "\n",
                "if 'google.colab' in str(get_ipython()):\n",
                "  print('Running on CoLab')\n",
                "  from google.colab import drive\n",
                "  drive.mount('/content/drive/')\n",
                "  DATASET_FILE_PATH = \"/content/drive/My Drive/Colab Notebooks/Dataset.npy\"\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "HUwxZvhVuoaB",
            "metadata": {
                "id": "HUwxZvhVuoaB",
                "tags": []
            },
            "outputs": [],
            "source": [
                "train_df: np.ndarray = np.load(DATASET_FILE_PATH)\n",
                "# TODO delete this when we fix performance\n",
                "train_df = train_df[:min(1000000, train_df.size)]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6ef26d1d-fe37-430c-8899-22953406249d",
            "metadata": {
                "id": "6ef26d1d-fe37-430c-8899-22953406249d"
            },
            "outputs": [],
            "source": [
                "#Define the splitter lambda function in order to tokenize the initial string data.\n",
                "splitter = lambda s: s.split(\",\")\n",
                "#Apply the splitter lambda function on the string np array\n",
                "train_df = np.array([splitter(x) for x in train_df])\n",
                "#Set the pickle file for storing the initial dataframe\n",
                "pickle_file = os.path.join(DATAFOLDER_PATH, \"dataframe.pkl\")\n",
                "#Check the existence of the specified file.\n",
                "if os.path.exists(pickle_file):\n",
                "    #Load the pickle file\n",
                "    dataframe = pd.read_pickle(pickle_file)\n",
                "else:\n",
                "    #Create the dataframe object.\n",
                "    dataframe = pd.DataFrame(train_df, columns=['User','Movie','Rating','Date'])\n",
                "    #Convert the string elements of the \"Users\" series into integers\n",
                "    dataframe[\"User\"] = dataframe[\"User\"].apply(lambda s:np.int64(s.replace(\"ur\",\"\")))\n",
                "    #Convert the string elements of the \"Movies\" series into integers\n",
                "    dataframe[\"Movie\"] = dataframe[\"Movie\"].apply(lambda s:np.int64(s.replace(\"tt\",\"\")))\n",
                "    #Convert the string elements of the \"Ratings\" series into integers\n",
                "    dataframe[\"Rating\"] = dataframe[\"Rating\"].apply(lambda s:np.int64(s))\n",
                "    #Convert the string element of \"Dates\" series into datetime Object\n",
                "    dataframe[\"Date\"] = pd.to_datetime(dataframe[\"Date\"])\n",
                "    dataframe.to_pickle(pickle_file)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8c5e2ea4-9604-4394-807d-1ede45da111f",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "8c5e2ea4-9604-4394-807d-1ede45da111f",
                "outputId": "e353bdda-ef48-4a1e-b74c-510c3c4ea18a"
            },
            "outputs": [],
            "source": [
                "#Get the unique users in the dataset.\n",
                "users = dataframe[\"User\"].unique()\n",
                "#Get the number of unique users\n",
                "users_num = len(users)\n",
                "#Get the unique movie items in the dataset.\n",
                "movies = dataframe[\"Movie\"].unique()\n",
                "#Get the number of unique movies\n",
                "movies_num = len(movies)\n",
                "#Get the total number of existing ratings.\n",
                "ratings_num = dataframe.shape[0]\n",
                "#Report the number of unique Users and Movies in the dataset\n",
                "print(\"INITIAL DATASET: {0} number of unique users and {1} of unique movies\".format(users_num, movies_num))\n",
                "#Report the total number of existing ratings in the dataset\n",
                "print(\"INITIAL DATASET: {} total number of existing ratings\".format(ratings_num))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "64464402-12c9-4080-a87d-6251f0d7beb5",
            "metadata": {
                "id": "64464402-12c9-4080-a87d-6251f0d7beb5"
            },
            "outputs": [],
            "source": [
                "#Define the pickle file that will store the time span per user dataframe\n",
                "pickle_file = os.path.join(DATAFOLDER_PATH, \"ratings_num_df.pkl\")\n",
                "#Check the existence of the previously defined pickle file\n",
                "if os.path.exists(pickle_file):\n",
                "    #Load the pickle file\n",
                "    ratings_num_df = pd.read_pickle(pickle_file)\n",
                "else:\n",
                "    ratings_num_df = dataframe.groupby(\"User\")[\"Rating\"].count().sort_values(ascending=False).reset_index(name=\"ratings_num\")\n",
                "    #Save the previously created dataframe to pickle\n",
                "    ratings_num_df.to_pickle(pickle_file)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f033c285-a2d3-4daf-9c96-8ebeb47f5103",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 573
                },
                "id": "f033c285-a2d3-4daf-9c96-8ebeb47f5103",
                "outputId": "7a92be6b-57d0-41fe-e05a-8e4818ae2206"
            },
            "outputs": [],
            "source": [
                "#Set the pickle file that will store the time span per user dataframe\n",
                "pickle_file = os.path.join(DATAFOLDER_PATH, \"ratings_span_df.pkl\")\n",
                "if os.path.exists(pickle_file):\n",
                "    ratings_span_df = pd.read_pickle(pickle_file)\n",
                "else:\n",
                "    ratings_span_df = dataframe.groupby(\"User\")[\"Date\"].apply(lambda date: max(date)-min(date)).sort_values(ascending=False).reset_index(name=\"ratings_span\")\n",
                "    ratings_span_df.to_pickle(pickle_file)\n",
                "#Create a new ratings dataframe by joining the previously defined dataframe\n",
                "ratings_df = ratings_num_df.join(ratings_span_df.set_index(\"User\"),on=\"User\")\n",
                "ratings_df[\"ratings_span\"]=ratings_df[\"ratings_span\"].dt.days\n",
                "#Set the threshold values for the minimum and maximum number of Ratings per user\n",
                "minimum_ratings = 100\n",
                "maximum_ratings = 300\n",
                "#Discard all users that do not pertain to the previous range of ratings\n",
                "reduced_ratings_df = ratings_df.loc[(ratings_df[\"ratings_num\"] >= minimum_ratings) & (ratings_df[\"ratings_num\"] <= maximum_ratings)]\n",
                "\n",
                "#Generate the frequency histogram for the number of ratings per user\n",
                "reduced_ratings_df[\"ratings_num\"].plot(kind='hist', title='Frequency of Ratings per User', xticks=range(minimum_ratings, maximum_ratings+1, 25))\n",
                "plt.xlabel('Frequency')\n",
                "plt.ylabel('Number of Users')\n",
                "\n",
                "plt.show()\n",
                "#Generate the frequency histogram for the time span of ratings per user\n",
                "reduced_ratings_df[\"ratings_span\"].plot(kind='hist', title='Frequency for time span of Ratings per User')\n",
                "plt.xlabel('Number of Users')\n",
                "plt.ylabel('Time range of Ratings (Days)')\n",
                "\n",
                "plt.show()\n",
                "\n",
                "\n",
                "\n",
                "                                                                                    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "49d13f9b-ae75-4625-a9ed-2c1f19bdaa7d",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "49d13f9b-ae75-4625-a9ed-2c1f19bdaa7d",
                "outputId": "7d2e1b2d-d38f-4af6-a1f5-12e0aed9032e"
            },
            "outputs": [],
            "source": [
                "#Get the final dataframe by excluding all users whose ratings fall outside the prespecified range\n",
                "final_df = dataframe.loc[dataframe[\"User\"].isin(reduced_ratings_df[\"User\"])].reset_index()\n",
                "#Drop the links (indices) to the original table\n",
                "final_df = final_df.drop(\"index\", axis=1)\n",
                "#Get the unique users and items in the final dataframe along with the final number of ratings\n",
                "final_users = final_df[\"User\"].unique()\n",
                "final_movies = final_df[\"Movie\"].unique()\n",
                "final_users_num = len(final_users)\n",
                "final_movies_num = len(final_movies)\n",
                "final_ratings_num = len(final_df)\n",
                "\n",
                "#Report the final number of unique users and movies in the dataset\n",
                "print(\"REDUCED DATASET: {0} number of unique users and {1} number of unique movies\".format(final_users_num, final_movies_num))\n",
                "#Report the final number of existing ratings in the dataset\n",
                "print(\"REDUCED DATASET: {} number of existing ratings in the dataset\".format(final_ratings_num))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ad131c64-257f-4f1b-9918-e005d7e96910",
            "metadata": {
                "id": "ad131c64-257f-4f1b-9918-e005d7e96910"
            },
            "outputs": [],
            "source": [
                "#We need to reset the users and items IDs in order to be able to construct a network of users and Movies. \n",
                "#Users and Movies IDs should be consecutive in the [1..final_users_num] and [1...final_movies_num]\n",
                "#Initially, we need to acquire the sorted versions of the user and movies\n",
                "sorted_final_users = np.sort(final_users)\n",
                "sorted_final_movies = np.sort(final_movies)\n",
                "#Generate the dictionary of final users as a mapping of the following \n",
                "#sorted_final_users --> [0...final_users_num-1]\n",
                "final_users_dict = dict(zip(sorted_final_users,list(range(0,final_users_num))))\n",
                "#Generate the dictionary of final items as a mapping of the following\n",
                "final_movies_dict = dict(zip(sorted_final_movies,list(range(0,final_movies_num))))\n",
                "#Apply the previously defined dictionary-based maps on the users and movies columns of the final dataframe\n",
                "final_df[\"User\"] = final_df[\"User\"].map(final_users_dict)\n",
                "final_df[\"Movie\"] = final_df[\"Movie\"].map(final_movies_dict)\n",
                "#Get a grouped version of the original dataframe based on the unique final users\n",
                "users_group_df = final_df.groupby(\"User\")\n",
                "#Initialize the adjacency matrix which stores the connection status for pair of users in the recommendation network\n",
                "W = np.zeros((final_users_num, final_users_num))\n",
                "#Iinitialize the matrix storing the number of commonly rated items for a pair of users\n",
                "CommonRatings = np.zeros((final_users_num, final_users_num))\n",
                "#Initialize the matrix of common ratings\n",
                "#Matrix W will be of size [final_users_num x final_users_num],\n",
                "#Let U = {u1, u2,...,un} be the final set of users and I = {i1,i2,...,im}\n",
                "#final set of movies. By considering the function Fi: U -> P(I) where\n",
                "#P(I) is the powerset of I, Fi(u) returns the subset of items that has been rated by user u. \n",
                "#In this context, the edge weight between any given pair of users (u,v) will be computed as:\n",
                "#\n",
                "#          |Intersection(Fi(u)),Fi(v))|\n",
                "#W(u,v) =  -----------------------------\n",
                "#               |Union(Fi(u),Fi(v))|\n",
                "#\n",
                "#\n",
                "#In order to speed up the construction of the adjacency matrix for the ratings network, \n",
                "#construct a dictionary object that will store a set of rated items for each unique user.\n",
                "user_items_dict = {}\n",
                "# for user in final_users:\n",
                "    #print(user)\n",
                "    # user_index = final_users_dict[user]\n",
                "    # user_movies = set(users_group_df.get_group(user_index)[\"Movie\"])\n",
                "    # user_items_dict[user_index] = user_movies\n",
                "                                                 \n",
                "# Initialize the dictionary for storing the set of rated items for each user\n",
                "user_items_dict = {}\n",
                "# print(final_users_dict)\n",
                "# print(sorted_final_users)\n",
                "# print(final_users_dict)\n",
                "# For each unique user, find the set of movies that they rated\n",
                "for user in final_users:\n",
                "    if user in final_users_dict:\n",
                "        user_index = final_users_dict[user]\n",
                "        user_movies = set(users_group_df.get_group(user_index)[\"Movie\"])\n",
                "        user_items_dict[user_index] = user_movies "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c0327c71-7ac0-496c-b53c-083ddbbc36de",
            "metadata": {
                "id": "c0327c71-7ac0-496c-b53c-083ddbbc36de"
            },
            "outputs": [],
            "source": [
                "user_ids = list(user_items_dict.keys())\n",
                "user_ids.sort()\n",
                "#Generate the sorted version of the dictionary\n",
                "user_items_dict = {user_index:user_items_dict[user_index] for user_index in user_ids}\n",
                "#Set the pickle file that will store the graph adjacency matrix W.\n",
                "pickle_file_weights = os.path.join(DATAFOLDER_PATH, \"w\")\n",
                "pickle_file_common_ratings = os.path.join(DATAFOLDER_PATH, \"common_ratings\")\n",
                "#Check the existence of the previously defined pickle file\n",
                "if os.path.exists(pickle_file_weights) & os.path.exists(pickle_file_common_ratings):\n",
                "    #Load the pickle file\n",
                "    W = np.load(pickle_file_weights)\n",
                "    CommonRatings = np.load(pickle_file_common_ratings)\n",
                "else:\n",
                "    for source_user in user_items_dict.keys():\n",
                "        for target_user in user_items_dict.keys():\n",
                "            intersection_items = user_items_dict[source_user].intersection(user_items_dict[target_user])\n",
                "            union_items = user_items_dict[source_user].union(user_items_dict[target_user])\n",
                "            W[source_user, target_user] = len(intersection_items)/len(union_items)\n",
                "            CommonRatings[source_user, target_user] = len(intersection_items)\n",
                "    np.save(pickle_file_weights,W)\n",
                "    np.save(pickle_file_common_ratings,CommonRatings)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2310ff25-7ad9-4cf6-b6f0-ada4575d6362",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "2310ff25-7ad9-4cf6-b6f0-ada4575d6362",
                "outputId": "dd2b7a9f-bd17-4ecb-987f-4861ec5ce53e"
            },
            "outputs": [],
            "source": [
                "W"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "77097013-5e4f-4e3c-832a-68b3f262e21f",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "77097013-5e4f-4e3c-832a-68b3f262e21f",
                "outputId": "64c0ff7a-29a6-4dac-af07-18e81025f84c"
            },
            "outputs": [],
            "source": [
                "CommonRatings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4959ffd5-63d3-4507-91da-77a2adbdd318",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 419
                },
                "id": "4959ffd5-63d3-4507-91da-77a2adbdd318",
                "outputId": "175ac22c-5f81-4853-e4b7-554a69142c9d"
            },
            "outputs": [],
            "source": [
                "final_df"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "Y7kiJK1IQUYL",
            "metadata": {
                "id": "Y7kiJK1IQUYL"
            },
            "source": [
                "#Δημιουργούμε έναν πίνακα χρηστών - ταινιών \n",
                "(οι χρήστες βρίσκονται στις γραμμές και οι ταινίες στις στήλες του πίνακα)\n",
                "όπου τα στοιχεία του πίνακα είναι από 1 - 10. Εάν ο χρήστης δεν έχει αξιολογήσει την ταινία,\n",
                "η αξιολόγηση που θα ανατεθεί είναι 0."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "903020b4-0070-41c3-90ef-46f8915f7a3e",
            "metadata": {
                "id": "903020b4-0070-41c3-90ef-46f8915f7a3e"
            },
            "outputs": [],
            "source": [
                "# Create a pivot table of user-movie ratings\n",
                "ratings_matrix = final_df.pivot_table(index='User', columns='Movie', values='Rating')\n",
                "ratings_matrix = ratings_matrix.fillna(0)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b15596e9-ec57-411d-952a-bc6bcd265942",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 450
                },
                "id": "b15596e9-ec57-411d-952a-bc6bcd265942",
                "outputId": "17d92724-d325-4128-c67e-6c7445711a2f"
            },
            "outputs": [],
            "source": [
                "ratings_matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3ceaf6a2-e9e7-4067-b01f-515bc34f59a9",
            "metadata": {
                "id": "3ceaf6a2-e9e7-4067-b01f-515bc34f59a9"
            },
            "outputs": [],
            "source": [
                "from pyclustering.cluster.kmeans import kmeans, kmeans_visualizer\n",
                "from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer\n",
                "from pyclustering.samples.definitions import FCPS_SAMPLES\n",
                "from pyclustering.utils import read_sample\n",
                "from pyclustering.cluster.kmeans import kmeans\n",
                "from pyclustering.utils.metric import type_metric, distance_metric"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "KvkvmxZ-QpTc",
            "metadata": {
                "id": "KvkvmxZ-QpTc"
            },
            "source": [
                "Θέλουμε να δημιουργήσουμε τον πίνακα βαρών \"λ\" των χρηστών. Τον πίνακα αξιολογήσεων δηλαδή όπου η τιμή της αξιολόγησης είναι 1 εάν η ταινία έχει αξιολογηθεί από τον χρήστη ή 0 εάν δεν έχει αξιολογηθεί"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a26dcef5-1f13-48b8-9445-74d3ce13a0ce",
            "metadata": {
                "id": "a26dcef5-1f13-48b8-9445-74d3ce13a0ce"
            },
            "outputs": [],
            "source": [
                "# Threshold\n",
                "threshold = 1\n",
                "\n",
                "# Transform to binary\n",
                "binary_matrix = np.where(ratings_matrix >= threshold, 1, 0)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "81271815-7f18-4890-a83a-8686c293dd30",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "81271815-7f18-4890-a83a-8686c293dd30",
                "outputId": "9a473204-7552-45c7-f506-c821dc9ea4d0"
            },
            "outputs": [],
            "source": [
                "binary_matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1f829cfd-4fa3-4ead-820f-2e13e332b22a",
            "metadata": {
                "id": "1f829cfd-4fa3-4ead-820f-2e13e332b22a"
            },
            "outputs": [],
            "source": [
                "# Convert the matrix to a numpy array\n",
                "matrix_array = ratings_matrix.to_numpy()\n",
                "\n",
                "# Create a dictionary that maps each row of the matrix to its index\n",
                "# matrix_dict = {tuple(row): i for i, row in enumerate(matrix_array)}"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "QJ0NaNSUrfyC",
            "metadata": {
                "id": "QJ0NaNSUrfyC"
            },
            "source": [
                "# ***Αλγόριθμοι Ομαδοποίησης Δεδομένων***"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "NJuh-Qpcya-i",
            "metadata": {
                "id": "NJuh-Qpcya-i"
            },
            "source": [
                "**Χρήση της Weighted Euclidean Distance**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "R8xa7QCu5SG9",
            "metadata": {
                "id": "R8xa7QCu5SG9"
            },
            "outputs": [],
            "source": [
                "from scipy.spatial.distance import pdist, cdist\n",
                "import numpy as np\n",
                "\n",
                "from scipy.sparse import csr_matrix\n",
                "\n",
                "def pairwise_weighted_euclidean_distance(X, weights):\n",
                "    # Find the indices of the rated movies for each pair of users\n",
                "    rated_movies = (weights_sparse.T @ weights_sparse) > 0\n",
                "\n",
                "    # Select only the rated movies for each pair of users\n",
                "    X_rated = X_sparse[:, rated_movies]\n",
                "    \n",
                "    # Calculate the pairwise weighted Euclidean distance between \n",
                "    #users who have rated the same movie\n",
                "    return cdist(X, metric='euclidean')\n",
                "\n",
                "def kmeans_pairwise_weighted_euclidean(X, weights, k, max_iters=2):\n",
                "\n",
                "    n, m = X.shape\n",
                "    centroids = X[np.random.choice(n, k, replace=False)]\n",
                "    distances = pairwise_weighted_euclidean_distance(X, weights)\n",
                "    for i in range(max_iters):\n",
                "        # Assign points to clusters\n",
                "        cluster_assignments = np.argmin(distances, axis=1)\n",
                "\n",
                "        # Recalculate cluster centroids\n",
                "        for j in range(k):\n",
                "            cluster_points = X[cluster_assignments == j]\n",
                "            if len(cluster_points) > 0:\n",
                "                centroids[j] = np.average(cluster_points, axis=0)\n",
                "\n",
                "        # Update distances to centroids\n",
                "        distances = pairwise_weighted_euclidean_distance(X, weights)\n",
                "\n",
                "    return cluster_assignments, centroids\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "wQx00tek0XXq",
            "metadata": {
                "id": "wQx00tek0XXq"
            },
            "source": [
                "# Clustering users using K-means\n",
                " We want to start by creating the symmetric D matrix which contains the pairwise weighted Euclidean distance for every pair of users.\n",
                " We calculate the distance between each user using \n",
                "*   dist_{u,v}=\\sum_{k=1}^{n}\\sqrt{|R_{u}(k) - R_{v}(k)|λ_{u}(k)λ_{v}(k)}\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a6911e7b-7bd6-4e50-9197-d57843b540bb",
            "metadata": {
                "id": "a6911e7b-7bd6-4e50-9197-d57843b540bb"
            },
            "outputs": [],
            "source": [
                "# Calculate the pairwise weighted Euclidean distance matrix\n",
                "# Calculate the pairwise weighted Euclidean distance matrix\n",
                "\n",
                "def create_euclidean_distance_matrix_cached(ratings_matrix: pd.DataFrame, binary_matrix: np_typing.NDArray) -> np_typing.NDArray[np.float64]:\n",
                "    #Set the npy file that will store the Euclidean distance matrix\n",
                "    npy_file = os.path.join(DATAFOLDER_PATH, \"euclidean_distance_matrix.npy\")\n",
                "    if os.path.exists(npy_file):\n",
                "        Dist_euclidean: np_typing.NDArray[np.float64] = np.load(npy_file, allow_pickle=True)\n",
                "        return Dist_euclidean\n",
                "    else:\n",
                "        n = ratings_matrix.shape[0]\n",
                "        Dist_euclidean = np.zeros((n, n))\n",
                "        for i in range(n):\n",
                "            for j in range(i, n):\n",
                "                d = np.sqrt(np.sum(binary_matrix[i,:]*binary_matrix[j,:] * (ratings_matrix.iloc[i,:] - ratings_matrix.iloc[j,:])**2))\n",
                "                Dist_euclidean[i,j] = d\n",
                "                Dist_euclidean[j,i] = d\n",
                "        np.save(npy_file, Dist_euclidean, allow_pickle=True, fix_imports=True)\n",
                "        return Dist_euclidean\n",
                "\n",
                "\n",
                "Dist_euclidean = create_euclidean_distance_matrix_cached(ratings_matrix, binary_matrix)\n",
                "Dist_euclidean"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "366a9ee9-1b48-46e5-903a-cf7c9f3a7f52",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 483
                },
                "id": "366a9ee9-1b48-46e5-903a-cf7c9f3a7f52",
                "outputId": "c575fd33-9592-4372-f242-fb1464690d9a"
            },
            "outputs": [],
            "source": [
                "df_euclidean = pd.DataFrame(Dist_euclidean)\n",
                "df_euclidean"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "NWWDOjPGRCYw",
            "metadata": {
                "id": "NWWDOjPGRCYw"
            },
            "source": [
                "Στον πίνακα αποστάσεων που έχουμε δημιουργήσει, θα τρέξουμε τον αλγόριθμο k-means ώστε να αποτιμήσουμε την ομοιότητα των χρηστών χρησιμοποιώντας τις μεταξύ τους αποστάσεις."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4c29300d-deb0-4ebd-b250-aa714a50a0a5",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "4c29300d-deb0-4ebd-b250-aa714a50a0a5",
                "outputId": "ab4babdc-1c59-418f-b762-ec8667ab038c"
            },
            "outputs": [],
            "source": [
                "from sklearn.cluster import KMeans\n",
                "# Cluster the users using K-means\n",
                "kmeans = KMeans(n_clusters=5).fit(Dist_euclidean)\n",
                "\n",
                "# Get the cluster labels\n",
                "labels_euclidean = kmeans.labels_\n",
                "\n",
                "# Print the labels\n",
                "print(labels_euclidean)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6hCiZ2wfG2_S",
            "metadata": {
                "id": "6hCiZ2wfG2_S"
            },
            "source": [
                "Cluster the users, by using a custom \n",
                "dist = 1 - np.abs(np.sum(R_u*R_v*weights_u*weights_l)/(np.sqrt(R^2_u*weights_u*weights_l)*np.sqrt(R^2_v*weights_u*weights_l)\n",
                "\n",
                "---\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2a2411e0-9be1-4d00-b1c0-7e58650da9bd",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "2a2411e0-9be1-4d00-b1c0-7e58650da9bd",
                "outputId": "979551cd-e0b5-4dd7-98be-bad9ac836fdd"
            },
            "outputs": [],
            "source": [
                "# Calculate the pairwise weighted Cosine distance matrix\n",
                "\n",
                "def create_cosine_distance_matrix_cached(ratings_matrix: pd.DataFrame, binary_matrix: np_typing.NDArray) -> np_typing.NDArray[np.float64]:\n",
                "    #Set the npy file that will store the Euclidean distance matrix\n",
                "    npy_file = os.path.join(DATAFOLDER_PATH, \"cosine_distance_matrix.npy\")\n",
                "    if os.path.exists(npy_file):\n",
                "        Dist_cosine: np_typing.NDArray[np.float64] = np.load(npy_file, allow_pickle=True)\n",
                "        return Dist_cosine\n",
                "    else:\n",
                "        n = ratings_matrix.shape[0]\n",
                "        Dist_cosine = np.zeros((n, n))\n",
                "        for i in range(n):\n",
                "            for j in range(i, n):\n",
                "                d = 1 - np.abs(np.sum(binary_matrix[i,:] * binary_matrix[j,:] * ratings_matrix.loc[i,:] * ratings_matrix.loc[j,:]) / (np.sqrt(np.sum(binary_matrix[i,:] * binary_matrix[j,:] * ratings_matrix.loc[i,:])* np.sqrt(np.sum(binary_matrix[i,:] * binary_matrix[j,:] * ratings_matrix.loc[j,:])))))\n",
                "                Dist_cosine[i,j] = d\n",
                "                Dist_cosine[j,i] = d\n",
                "        np.save(npy_file, Dist_cosine, allow_pickle=True, fix_imports=True)\n",
                "        return Dist_cosine\n",
                "\n",
                "\n",
                "Dist_cosine = create_cosine_distance_matrix_cached(ratings_matrix, binary_matrix)\n",
                "Dist_cosine"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8cOc0082IqT2",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 483
                },
                "id": "8cOc0082IqT2",
                "outputId": "4fab0eb2-5634-4558-c70a-6f3c33a44bc4"
            },
            "outputs": [],
            "source": [
                "df_cosine = pd.DataFrame(Dist_cosine)\n",
                "df_cosine = df_cosine.replace(np.nan, 0)\n",
                "df_cosine"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9IvMaCXORgXf",
            "metadata": {
                "id": "9IvMaCXORgXf"
            },
            "source": [
                "Στον πίνακα αποστάσεων που έχουμε δημιουργήσει, θα τρέξουμε τον αλγόριθμο k-means ώστε να αποτιμήσουμε την ομοιότητα των χρηστών χρησιμοποιώντας τις μεταξύ τους αποστάσεις."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1K3Ry-W0IN74",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "1K3Ry-W0IN74",
                "outputId": "afd594b6-b08c-47dd-8b9a-7a3c0d7baa4c"
            },
            "outputs": [],
            "source": [
                "# Cluster the users using K-means\n",
                "kmeans = KMeans(n_clusters=5).fit(df_cosine)\n",
                "\n",
                "# Get the cluster labels\n",
                "labels_cosine = kmeans.labels_\n",
                "\n",
                "# Print the labels\n",
                "print(labels_cosine)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "s_MLiRK6Rh6E",
            "metadata": {
                "id": "s_MLiRK6Rh6E"
            },
            "source": [
                "# Elbow Method\n",
                "Χρησιμοποιούμε την elbow method ώστε να επιλέξουμε τον βέλτιστο αριθμό clusters στον οποίο θα διαχωριστούν τα δεδομένα χρησιμοποιώντας τον k-means"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9ITnxc3WJec5",
            "metadata": {
                "id": "9ITnxc3WJec5"
            },
            "outputs": [],
            "source": [
                "def elbow_method(df: pd.DataFrame, max_iter: int):\n",
                "  distortions = []\n",
                "  K = range(1,max_iter)\n",
                "  for k in K:\n",
                "    kmeanModel = KMeans(n_clusters=k)\n",
                "    kmeanModel.fit(df)\n",
                "    distortions.append(kmeanModel.inertia_)\n",
                "  plt.figure(figsize=(16,8))\n",
                "  plt.plot(K, distortions, 'bx-')\n",
                "  plt.xlabel('k')\n",
                "  plt.ylabel('Distortion')\n",
                "  plt.title('The Elbow Method showing the optimal k')\n",
                "  plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "UJgM52L7G1NU",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 839
                },
                "id": "UJgM52L7G1NU",
                "outputId": "4b0d6259-6797-4d2d-c680-524c5f42b65e"
            },
            "outputs": [],
            "source": [
                "#Using the elbow method on Cosine distance\n",
                "elbow_method(df_cosine, 10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "zOCZ4XZyHiKU",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 839
                },
                "id": "zOCZ4XZyHiKU",
                "outputId": "d2e09fda-e558-4938-c4ed-235d320fa669"
            },
            "outputs": [],
            "source": [
                "#Using the elbow method on Euclidean distance\n",
                "elbow_method(Dist_euclidean, 10)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "P1K1uNDHW3m4",
            "metadata": {
                "id": "P1K1uNDHW3m4"
            },
            "source": [
                "First, we have to modify our df in order to keep the first n users and assign our labels to them"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2oALEUXvXAqJ",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 450
                },
                "id": "2oALEUXvXAqJ",
                "outputId": "e618bf0c-d11b-4d8d-f3a0-33fbcf1e6f79"
            },
            "outputs": [],
            "source": [
                "# ratings_matrix = ratings_matrix.head(100)\n",
                "ratings_matrix"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "AYNf6PMYP-QC",
            "metadata": {
                "id": "AYNf6PMYP-QC"
            },
            "source": [
                "Next, we'll use the PCA method in order to reduce the dimensionality of our matrix and plot our clusters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ejOUlMYaQOEV",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "ejOUlMYaQOEV",
                "outputId": "b43b9fc9-b929-4f25-927d-08d4dc0feb2d"
            },
            "outputs": [],
            "source": [
                "from sklearn.decomposition import PCA\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "# instantiate StandardScaler and PCA with 2 components for 2D scatter plot\n",
                "scaler = StandardScaler()\n",
                "pca = PCA(n_components=2)\n",
                "\n",
                "# fit and transform the ratings matrix\n",
                "ratings_pca = pca.fit_transform(ratings_matrix)\n",
                "\n",
                "# print the explained variance ratio for each component\n",
                "print(pca.explained_variance_ratio_)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "UCxdIcygTFor",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 419
                },
                "id": "UCxdIcygTFor",
                "outputId": "9598b8eb-ba75-4ff4-f20c-6163f837bf44"
            },
            "outputs": [],
            "source": [
                "# create a new dataframe with the PCA components and user index\n",
                "df_pca = pd.DataFrame(ratings_pca, index=range(0, ratings_matrix.shape[0]))\n",
                "df_pca['Cluster'] = labels_euclidean\n",
                "df_pca"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a4u_fFobUb_k",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 295
                },
                "id": "a4u_fFobUb_k",
                "outputId": "6705f8a0-0a7c-4355-cff7-19e32b0471a5",
                "tags": []
            },
            "outputs": [],
            "source": [
                "# create a scatter plot of the PCA components\n",
                "fig, ax = plt.subplots()\n",
                "\n",
                "for label, color in zip(df_pca['Cluster'].unique(), ['blue', 'red', 'green', 'orange', 'purple']):\n",
                "    group = df_pca.groupby('Cluster').get_group(label)\n",
                "    ax.scatter(group[0], group[1], c=color, label=f'Cluster {label}')\n",
                "\n",
                "# set the axis labels and title\n",
                "ax.set_xlabel('Component 1')\n",
                "ax.set_ylabel('Component 2')\n",
                "ax.set_title('PCA Transformed User-Movie Ratings')\n",
                "\n",
                "# add a legend\n",
                "ax.legend()\n",
                "\n",
                "# show the plot\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "yOX_X78d1Wxg",
            "metadata": {
                "id": "yOX_X78d1Wxg"
            },
            "source": [
                "Να σχολιάσετε την αποτελεσματικότητα των συγκεκριμένων μετρικών στην αποτίμηση της ομοιότητας μεταξύ ενός ζεύγους διανυσμάτων προτιμήσεων χρηστών R_u και R_v.\n",
                "\n",
                "Για την μετρική της ευκλείδιας απόστασης: \n",
                "\n",
                "\n",
                "*   Η ομοιότητα των χρηστών είναι **αντιστρόφως ανάλογη** της απόστασης μεταξύ τους.\n",
                "*   Για να έχουμε αποτέλεσμα, θα πρέπει να υπάρχει **επικάλυψη μεταξύ των χρηστών.** Πρέπει δηλαδή να έχουν αξιολογήσει κοινές ταινίες.\n",
                "*   Ο υπολογισμός του k-means γίνεται πολύ πιο υπολογιστικά εντατικός λόγω των εκτεταμένων πολλαπλασιασμών πινάκων που εκτελείται.\n",
                "\n",
                "\n",
                "---\n",
                "\n",
                "\n",
                "Για την μετρική του συνημιτόνου:\n",
                "\n",
                "\n",
                "1.   Για να έχουμε αποτέλεσμα, θα πρέπει να υπάρχει **επικάλυψη μεταξύ των χρηστών.** Πρέπει δηλαδή να έχουν αξιολογήσει κοινές ταινίες.\n",
                "2.   Ο υπολογισμός του k-means γίνεται πολύ πιο υπολογιστικά εντατικός λόγω των εκτεταμένων πολλαπλασιασμών πινάκων που εκτελείται.\n",
                "3.   Η ομοιότητα των χρηστών μπορεί να υπολογιστεί στην περίπτωση που είναι η γωνία μεταξύ των διανυσμάτων τους από 0 - 90 ως ομοιότητα ενώ από 90 - 180 μπορούμε να εκφράσουμε την αντίθεση των χρηστών. Οπότε σε κάθε περίπτωση η μετρική μας βοηθά να ομαδοποιήσουμε τους χρήστες.\n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "qunv_c5V3qgC",
            "metadata": {
                "id": "qunv_c5V3qgC"
            },
            "source": [
                "##JACCARD DISTANCE\n",
                "Η απόσταση Jaccard απομετρά τη **διαφορετικότητα** μεταξύ δύο συνόλων (στην περίπτωσή μας δύο χρηστών). \n",
                "\n",
                "\n",
                "\n",
                "*   Στην περίπτωση που η τομή των δύο χρηστών γίνει μηδέν (δεν υπάρχουν δηλαδή κοινά αξιολογήσιμες ταινίες) η διαφορετικότητα των χρηστών παίρνει τη μέγιστη τιμή της, 1\n",
                "*   Η διαφορετικότητα των χρηστών θα γίνει **ελάχιστη** όταν η *τομή* των δύο χρηστών είναι ίση με την *ένωσή* τους, όταν δηλαδή τα δύο σύνολα γίνουν *ίσα*\n",
                "*   Μπορεί να χρησιμοποιηθεί για τη σύγκριση της ομοιότητας οποιουδήποτε είδους δεδομένων, συμπεριλαμβανομένων δεδομένων χρονοσειρών, φωτογραφιών, κειμένου και εικόνων.\n",
                "\n",
                "\n",
                "---\n",
                "\n",
                "\n",
                "Κάποια από τα μειονεκτήματα της ανωτέρω μετρικής είναι τα ακόλουθα:\n",
                "\n",
                "\n",
                "---\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "*   **Απουσία \"βαρών\"**: Η απόσταση Jaccard εξετάζει μόνο την παρουσία ή την απουσία αξιολογήσεων για κάθε χρήστη και δεν λαμβάνει υπόψη τις πραγματικές τιμές αξιολόγησης. Μπορεί δηλαδή η *διαφορετικότητα*, η τιμή δηλαδή που θα προκύψει από την απόσταση Jaccard δύο χρηστών να είναι ελάχιστη, εάν έχουν αξιολογήσει τις ίδιες ταινίες ακόμα και αν ο ένας τις έχει αξιολογήσει με 5 και ο άλλος με 1.\n",
                "*   **Αραιότητα αξιολογήσεων**: Για παράδειγμα, εάν δύο χρήστες έχουν αξιολογήσει μόνο έναν μικρό αριθμό ταινιών, είναι πιθανό να μην έχουν αξιολογήσει καμία από τις ίδιες ταινίες, άρα η τομή τους θα είναι μηδέν, με αποτέλεσμα η *διαφορετικότητά* τους να είναι μέγιστη, ακόμη και αν οι προτιμήσεις τους για τις ταινίες είναι στην πραγματικότητα αρκετά παρόμοιες. \n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "E2udjYdk7qUs",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "E2udjYdk7qUs",
                "outputId": "7386e9fa-6a01-46d9-aedb-6228fd293923"
            },
            "outputs": [],
            "source": [
                "jaccard_dist = 1 - W\n",
                "print(jaccard_dist)\n",
                "jaccard_df = pd.DataFrame(jaccard_dist)\n",
                "jaccard_df\n",
                "\n",
                "def kmeans_clustering(jaccard_dist, k):\n",
                "    # Initialize k-means object\n",
                "    kmeans = KMeans(n_clusters=k)\n",
                "\n",
                "    # Fit the k-means object to the Jaccard distance matrix\n",
                "    kmeans.fit(jaccard_dist)\n",
                "\n",
                "    return kmeans.labels_\n",
                "\n",
                "labels = kmeans_clustering(jaccard_dist, 5)\n",
                "print(labels)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "Xt4Rb8BSTKlJ",
            "metadata": {
                "id": "Xt4Rb8BSTKlJ"
            },
            "outputs": [],
            "source": [
                "def plot_pca_cluster(ratings_matrix, n_clusters):\n",
                "    # instantiate StandardScaler and PCA with 2 components for 2D scatter plot\n",
                "    scaler = StandardScaler()\n",
                "    pca = PCA(n_components=2)\n",
                "\n",
                "    # fit and transform the ratings matrix\n",
                "    ratings_pca = pca.fit_transform(ratings_matrix)\n",
                "\n",
                "    # apply K-means clustering\n",
                "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
                "    labels = kmeans.fit_predict(ratings_matrix)\n",
                "\n",
                "    # create a new dataframe with the PCA components and cluster labels\n",
                "    df_pca = pd.DataFrame(ratings_pca, index=range(0, ratings_matrix.shape[0]), columns=['Component 1', 'Component 2'])\n",
                "    df_pca['Cluster'] = labels\n",
                "\n",
                "    # create a scatter plot of the PCA components with color-coded clusters\n",
                "    fig, ax = plt.subplots()\n",
                "\n",
                "    for label, color in zip(df_pca['Cluster'].unique(), ['blue', 'red', 'green', 'orange', 'purple']):\n",
                "        group = df_pca.groupby('Cluster').get_group(label)\n",
                "        ax.scatter(group['Component 1'], group['Component 2'], c=color, label=f'Cluster {label}')\n",
                "\n",
                "    # set the axis labels and title\n",
                "    ax.set_xlabel('Component 1')\n",
                "    ax.set_ylabel('Component 2')\n",
                "    ax.set_title('PCA Transformed User-Movie Ratings')\n",
                "\n",
                "    # add a legend\n",
                "    ax.legend()\n",
                "\n",
                "    # show the plot\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "_fHEwfARTLtN",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 349
                },
                "id": "_fHEwfARTLtN",
                "outputId": "80abb934-dcac-4a0f-9824-e0094472b3a5"
            },
            "outputs": [],
            "source": [
                "plot_pca_cluster(jaccard_dist, 5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f9f5232e",
            "metadata": {},
            "outputs": [],
            "source": [
                "CommonRatings"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "9bb66f3f",
            "metadata": {},
            "source": [
                "# THANOS NEURAL NETWORK"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "17040c42",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define functions to create and train a linear regression model\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "\n",
                "def create_nn_original_df(NEIGHBOURS, NEIGHBOURS_COLUMNS: list[str]):\n",
                "    # create user_ratings_list\n",
                "    user_index_list: list[int] = []\n",
                "    user_ratings_list: list[int] = []\n",
                "    for i in range(NEIGHBOURS.shape[0]):\n",
                "        user_index_list.extend([i for ratings in ratings_matrix[i]])\n",
                "        user_ratings_list.extend(ratings_matrix[i])\n",
                "        \n",
                "    # create neighbors list\n",
                "    neighbors_list: list[list[int]] = []\n",
                "    for i in range(NEIGHBOURS.shape[1]):\n",
                "        neighbor_ratings_list: list[int] = []\n",
                "\n",
                "        for j in range(NEIGHBOURS.shape[0]):\n",
                "            neighbor = NEIGHBOURS[j][i]\n",
                "            neighbor_ratings_list.extend(ratings_matrix[neighbor])\n",
                "\n",
                "        neighbors_list.append(neighbor_ratings_list)\n",
                "\n",
                "\n",
                "    nn_origin_df = pd.DataFrame()\n",
                "    nn_origin_df['USER_INDEX'] = user_index_list\n",
                "    nn_origin_df['USER_RATINGS'] = user_ratings_list\n",
                "\n",
                "    for i in range(len(neighbors_list)):\n",
                "        neighbor_ratings_list: list[int] = neighbors_list[i]\n",
                "        nn_origin_df[NEIGHBOURS_COLUMNS[i]] = neighbor_ratings_list\n",
                "\n",
                "\n",
                "    return nn_origin_df\n",
                "\n",
                "\n",
                "def create_nn_filtered_normalized_df(ratings_normalize_factor, NEIGHBOURS, NEIGHBOURS_COLUMNS: list[str]):\n",
                "    nn_origin_df = create_nn_original_df(NEIGHBOURS, NEIGHBOURS_COLUMNS)\n",
                "    display('Dataframe with Original user ratings and neighbors ratings')\n",
                "    display(nn_origin_df)\n",
                "    display(nn_origin_df.describe())\n",
                "\n",
                "    # create filtered dataframe\n",
                "\n",
                "    nn_filtered_df = nn_origin_df.copy()[nn_origin_df['USER_RATINGS'] != 0]\n",
                "    display('Dataframe with filter user ratings (non zero) and neighbors ratings')\n",
                "    display(nn_filtered_df)\n",
                "    display(nn_filtered_df.describe())\n",
                "\n",
                "    nn_filtered_normalized_df = nn_filtered_df.copy()\n",
                "    columns_to_normalize = ['USER_RATINGS']\n",
                "    columns_to_normalize.extend(NEIGHBOURS_COLUMNS)\n",
                "    nn_filtered_normalized_df[columns_to_normalize] = nn_filtered_normalized_df[columns_to_normalize] / ratings_normalize_factor\n",
                "    display('Dataframe with filter user ratings (non zero) and neighbors ratings scaled by maximum rating')\n",
                "    display(nn_filtered_normalized_df)\n",
                "    display(nn_filtered_normalized_df.describe())\n",
                "    return nn_origin_df, nn_filtered_df, nn_filtered_normalized_df\n",
                "\n",
                "\n",
                "def create_feature_columns(NEIGHBOURS_COLUMNS: list[str]):\n",
                "    \"\"\"Create feature columns\"\"\"\n",
                "    feature_columns = []\n",
                "    for column in NEIGHBOURS_COLUMNS:\n",
                "        feature_columns.append(tf.feature_column.numeric_column(column))\n",
                "\n",
                "    return feature_columns\n",
                "\n",
                "\n",
                "def create_model(my_learning_rate, feature_columns):\n",
                "    \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
                "    # Most simple tf.keras models are sequential.\n",
                "    model = tf.keras.models.Sequential()\n",
                "    \n",
                "    # Add the layer containing the feature columns to the model.\n",
                "    model.add(tf.keras.layers.DenseFeatures(feature_columns))\n",
                "\n",
                "    # Implement L2 regularization in the first hidden layer.\n",
                "    model.add(tf.keras.layers.Dense(units=20, \n",
                "                                    activation='relu',\n",
                "                                    kernel_regularizer=tf.keras.regularizers.l2(0.04),\n",
                "                                    name='Hidden1'))\n",
                "    \n",
                "    # Implement L2 regularization in the second hidden layer.\n",
                "    model.add(tf.keras.layers.Dense(units=12, \n",
                "                                    activation='relu', \n",
                "                                    kernel_regularizer=tf.keras.regularizers.l2(0.04),\n",
                "                                    name='Hidden2'))\n",
                "\n",
                "    # Define the output layer.\n",
                "    model.add(tf.keras.layers.Dense(units=1,  \n",
                "                                    name='Output'))                              \n",
                "    \n",
                "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=my_learning_rate),\n",
                "                    loss=\"mean_squared_error\",\n",
                "                    metrics=[tf.keras.metrics.MeanSquaredError(), tf.keras.metrics.MeanAbsoluteError()])\n",
                "    return model           \n",
                "\n",
                "\n",
                "def train_model(model: tf.keras.models.Sequential, train_df: pd.DataFrame, NEIGHBOURS_COLUMNS: list[str], epochs: int, batch_size: int=1):\n",
                "    \"\"\"Train the model by feeding it data.\"\"\"\n",
                "\n",
                "    # Features as a dictionary with key the neighbor ratings column name and value an ndarray of the values\n",
                "    features = {\n",
                "        column: np.array(train_df[column]) for column in NEIGHBOURS_COLUMNS\n",
                "    }\n",
                "    label = np.array(train_df['USER_RATINGS'])\n",
                "    history = model.fit(x=features, y=label, batch_size=batch_size, epochs=epochs, shuffle=True) \n",
                "\n",
                "    # The list of epochs is stored separately from the rest of history.\n",
                "    epochs = history.epoch\n",
                "    \n",
                "    # To track the progression of training, gather a snapshot\n",
                "    # of the model's mean squared error at each epoch. \n",
                "    hist = pd.DataFrame(history.history)\n",
                "    mse = hist[\"mean_squared_error\"]\n",
                "    mae = hist[\"mean_absolute_error\"]\n",
                "\n",
                "    return epochs, mse, mae\n",
                "\n",
                "\n",
                "def evaluate_model(model: tf.keras.models.Sequential, test_df: pd.DataFrame, NEIGHBOURS_COLUMNS: list[str], batch_size: int=1):\n",
                "    \"\"\"Evaluate the model with the test_df\"\"\"\n",
                "\n",
                "    # Features as a dictionary with key the neighbor ratings column name and value an ndarray of the values\n",
                "    features = {\n",
                "        column: np.array(test_df[column]) for column in NEIGHBOURS_COLUMNS\n",
                "    }\n",
                "    label = np.array(test_df['USER_RATINGS'])\n",
                "    return model.evaluate(x = features, y = label, batch_size=batch_size)\n",
                "\n",
                "\n",
                "def predict_model(model: tf.keras.models.Sequential, test_df: pd.DataFrame, NEIGHBOURS_COLUMNS: list[str], batch_size: int=1):\n",
                "    \"\"\"Evaluate the model with the test_df\"\"\"\n",
                "\n",
                "    # Features as a dictionary with key the neighbor ratings column name and value an ndarray of the values\n",
                "    features = {\n",
                "        column: np.array(test_df[column]) for column in NEIGHBOURS_COLUMNS\n",
                "    }\n",
                "    return model.predict(x = features, batch_size=batch_size)\n",
                "\n",
                "\n",
                "def plot_the_loss_curve(epochs, mse_or_mae, is_mse: bool):\n",
                "    \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
                "\n",
                "    plt.figure()\n",
                "    plt.xlabel(\"Epoch\")\n",
                "    ylabel = 'Train Mean Squared Error' if is_mse else 'Train Mean Absolute Error'\n",
                "    plt.ylabel(ylabel)\n",
                "\n",
                "    plt.plot(epochs, mse_or_mae, label=\"Loss\")\n",
                "    plt.legend()\n",
                "    plt.ylim([mse_or_mae.min()*0.95, mse_or_mae.max() * 1.03])\n",
                "    plt.show()\n",
                "\n",
                "\n",
                "\n",
                "def calculate_real_mse_mae(origin_ratings: np_typing.NDArray, predictions: np_typing.NDArray):\n",
                "    \"\"\"Calculate the real mse and mae comparing real ratings and predictions.\"\"\"\n",
                "\n",
                "    n = 0\n",
                "    absolute_sum = 0\n",
                "    squared_sum = 0\n",
                "    for i in range(origin_ratings.shape[0]):\n",
                "        # take into consideration only non 0 ratings\n",
                "        if origin_ratings[i] != 0.0:\n",
                "            n += 1\n",
                "            abs_value = abs(origin_ratings[i] - predictions[i])\n",
                "            absolute_sum += abs_value\n",
                "            squared_sum += math.sqrt(abs_value)\n",
                "\n",
                "    \n",
                "    mse = absolute_sum / n\n",
                "    mae = squared_sum / n\n",
                "    return mse, mae\n",
                "\n",
                "\n",
                "def create_train_evaluate_neural_network(ratings_normalize_factor, nn_origin_df: pd.DataFrame, nn_filtered_normalized_df: pd.DataFrame, NEIGHBOURS_COLUMNS: list[str]) -> tuple[float, float, float, float, float, float]:\n",
                "    train_df, test_df = train_test_split(nn_filtered_normalized_df, test_size=0.2, random_state=42)\n",
                "    train_df = pd.DataFrame(train_df)\n",
                "    test_df = pd.DataFrame(test_df)\n",
                "\n",
                "    display('traind_df', train_df)\n",
                "    display('test_df', test_df)\n",
                "\n",
                "    # The following variables are the hyperparameters.\n",
                "    learning_rate = 0.01\n",
                "    epochs = 32\n",
                "    batch_size = 96\n",
                "\n",
                "    # define the feature columns\n",
                "    feature_columns = create_feature_columns(NEIGHBOURS_COLUMNS)\n",
                "    # Establish the model's topography.\n",
                "    my_model = create_model(learning_rate, feature_columns)\n",
                "\n",
                "    # Train the model on the normalized training set.\n",
                "    display('Training the model with the train_df')\n",
                "    epochs, train_mse_series, train_mae_series = train_model(my_model, train_df, NEIGHBOURS_COLUMNS, epochs, batch_size)\n",
                "    train_mse = train_mse_series.iloc[-1]\n",
                "    train_mae = train_mae_series.iloc[-1]\n",
                "    plot_the_loss_curve(epochs, train_mse_series, True)\n",
                "    plot_the_loss_curve(epochs, train_mae_series, False)\n",
                "\n",
                "\n",
                "    display('Evaluating the model against the test_df')\n",
                "    test_loss, test_mse, test_mae = evaluate_model(my_model, test_df, NEIGHBOURS_COLUMNS, batch_size)\n",
                "    display('test_loss, test_mse, test_mae', test_loss, test_mse, test_mae)\n",
                "\n",
                "    predictions_normalized = predict_model(my_model, nn_origin_df, NEIGHBOURS_COLUMNS, batch_size)\n",
                "    # turn list of sinle item lists to a sigle list with floats\n",
                "    predictions_normalized = np.array([prediction_normalized[0] for prediction_normalized in predictions_normalized])\n",
                "    predictions = predictions_normalized * ratings_normalize_factor\n",
                "    display('predictions', predictions)\n",
                "    display('predictions.shape', predictions.shape)\n",
                "\n",
                "    real_ratings = nn_origin_df['USER_RATINGS'].to_numpy()\n",
                "    display('real_ratings', real_ratings)\n",
                "    real_mse, real_mae = calculate_real_mse_mae(real_ratings, predictions_normalized)\n",
                "    display(f'real_mean_squared_error={real_mse}, real_mean_absolute_error={real_mae}')\n",
                "\n",
                "    return train_mse, train_mae, test_mse, test_mae, real_mse, real_mae\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4ab448ca",
            "metadata": {},
            "outputs": [],
            "source": [
                "# THANOS NEURAL NETWORK\n",
                "\n",
                "n_clusters = 5\n",
                "k = 5\n",
                "ratings_normalize_factor: float = ratings_matrix.max().max()\n",
                "\n",
                "NEIGHBOURS = np.array([\n",
                "    [np.random.randint(low=0, high=k) for j in range(k)] for u in range(CommonRatings.shape[0])\n",
                "])\n",
                "display('ratings_normalize_factor', ratings_normalize_factor)\n",
                "display('NEIGHBOURS.shape', NEIGHBOURS.shape)\n",
                "display('NEIGHBOURS', NEIGHBOURS)\n",
                "\n",
                "# Create, train and evaluate a Neural Network for each cluster\n",
                "\n",
                "NEIGHBOURS_COLUMNS = [f'NEIGHBOR_RATINGS_{i}' for i in range(k)]\n",
                "results: list[list[float]] = []\n",
                "results_df_index: list[str] = []\n",
                "\n",
                "for cluster_index in range(1):\n",
                "    nn_origin_df, nn_filtered_df, nn_filtered_normalized_df = create_nn_filtered_normalized_df(\n",
                "        ratings_normalize_factor, NEIGHBOURS, NEIGHBOURS_COLUMNS)\n",
                "\n",
                "    train_mse, train_mae, test_mse, test_mae, real_mse, real_mae = create_train_evaluate_neural_network(\n",
                "        ratings_normalize_factor, nn_origin_df, nn_filtered_normalized_df, NEIGHBOURS_COLUMNS)\n",
                "    results.append([train_mse, train_mae, test_mse,\n",
                "                   test_mae, real_mse, real_mae])\n",
                "    results_df_index.append(f'CLUSTER_{cluster_index}')\n",
                "\n",
                "\n",
                "results_df_columns = ['TRAIN_MSE', 'TRAIN_MAE',\n",
                "                      'TEST_MSE', 'TEST_MAE', 'REAL_MSE', 'REAL_MAE']\n",
                "results_df = pd.DataFrame(\n",
                "    results, columns=results_df_columns, index=results_df_index)\n",
                "display('results_df', results_df)\n"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "include_colab_link": true,
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.2"
        },
        "toc-autonumbering": true,
        "toc-showcode": true,
        "toc-showmarkdowntxt": false,
        "toc-showtags": false
    },
    "nbformat": 4,
    "nbformat_minor": 5
}

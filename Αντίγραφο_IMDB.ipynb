{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThanosApostolou/aics-pattern-recognition/blob/main/%CE%91%CE%BD%CF%84%CE%AF%CE%B3%CF%81%CE%B1%CF%86%CE%BF_IMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63e0249e",
      "metadata": {
        "id": "63e0249e"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThanosApostolou/aics-pattern-recognition/blob/main/IMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1oA4US2rVw3X",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oA4US2rVw3X",
        "outputId": "2f3f2100-86bf-4bf8-e80c-1bd6c79c4f39",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# INSTALL DEPENDENCIES\n",
        "# Uncomment and run only once.\n",
        "%pip install matplotlib numpy pandas scikit-learn scipy tensorflow pyclustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a24235d-b5ff-4fed-8231-bbaf33d6c772",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a24235d-b5ff-4fed-8231-bbaf33d6c772",
        "outputId": "d65dd253-1b85-4a58-d988-98efca5b7df4",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# IMPORTS AND GLOBAL CONSTANTS\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "import math\n",
        "import tensorflow as tf\n",
        "import datetime, os\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import typing\n",
        "import numpy.typing as np_typing\n",
        "from sklearn.model_selection import train_test_split\n",
        "##MAIN PROGRAM VARIABLES##\n",
        "##(0): dataset: np array of strings\n",
        "##(1): dataframe: original dataset in its primal form\n",
        "##(2): ratings_num_df: new dataframe storing the number of rated items per unique user\n",
        "##(3): ratings_span_df: new dataframe storing the timespan in days for each user\n",
        "##(4): minimum_ratings - maximum_ratings => ratings_df=> (i) final_df\n",
        "\n",
        "# Constants\n",
        "DATASET_FILE_PATH = \"./Dataset.npy\"\n",
        "#Define the figures path\n",
        "FIGURES_PATH = \"figures\"\n",
        "os.makedirs(FIGURES_PATH, exist_ok=True)\n",
        "# #Define the data folder path\n",
        "DATAFOLDER_PATH = \"datafiles\"\n",
        "os.makedirs(DATAFOLDER_PATH, exist_ok=True)\n",
        "\n",
        "L_CLUSTERS_NUM = 5\n",
        "K_NEIGHBORS_NUM = 5\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  print('Running on CoLab')\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive/')\n",
        "  DATASET_FILE_PATH = \"/content/drive/My Drive/Colab Notebooks/Dataset.npy\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HUwxZvhVuoaB",
      "metadata": {
        "id": "HUwxZvhVuoaB",
        "tags": []
      },
      "outputs": [],
      "source": [
        "dataset: np.ndarray = np.load(DATASET_FILE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ef26d1d-fe37-430c-8899-22953406249d",
      "metadata": {
        "id": "6ef26d1d-fe37-430c-8899-22953406249d"
      },
      "outputs": [],
      "source": [
        "#Define the splitter lambda function in order to tokenize the initial string data.\n",
        "splitter = lambda s: s.split(\",\")\n",
        "#Apply the splitter lambda function on the string np array\n",
        "dataset = np.array([splitter(x) for x in dataset])\n",
        "#Set the pickle file for storing the initial dataframe\n",
        "pickle_file = os.path.join(DATAFOLDER_PATH, \"dataframe.pkl\")\n",
        "#Check the existence of the specified file.\n",
        "if os.path.exists(pickle_file):\n",
        "    #Load the pickle file\n",
        "    dataframe = pd.read_pickle(pickle_file)\n",
        "else:\n",
        "    #Create the dataframe object.\n",
        "    dataframe = pd.DataFrame(dataset, columns=['User','Movie','Rating','Date'])\n",
        "    #Convert the string elements of the \"Users\" series into integers\n",
        "    dataframe[\"User\"] = dataframe[\"User\"].apply(lambda s:np.int64(s.replace(\"ur\",\"\")))\n",
        "    #Convert the string elements of the \"Movies\" series into integers\n",
        "    dataframe[\"Movie\"] = dataframe[\"Movie\"].apply(lambda s:np.int64(s.replace(\"tt\",\"\")))\n",
        "    #Convert the string elements of the \"Ratings\" series into integers\n",
        "    dataframe[\"Rating\"] = dataframe[\"Rating\"].apply(lambda s:np.int64(s))\n",
        "    #Convert the string element of \"Dates\" series into datetime Object\n",
        "    dataframe[\"Date\"] = pd.to_datetime(dataframe[\"Date\"])\n",
        "    dataframe.to_pickle(pickle_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c5e2ea4-9604-4394-807d-1ede45da111f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c5e2ea4-9604-4394-807d-1ede45da111f",
        "outputId": "dcb0cd0b-fe00-4e3a-9c2c-3e3485fece02"
      },
      "outputs": [],
      "source": [
        "#Get the unique users in the dataset.\n",
        "users = dataframe[\"User\"].unique()\n",
        "#Get the number of unique users\n",
        "users_num = len(users)\n",
        "#Get the unique movie items in the dataset.\n",
        "movies = dataframe[\"Movie\"].unique()\n",
        "#Get the number of unique movies\n",
        "movies_num = len(movies)\n",
        "#Get the total number of existing ratings.\n",
        "ratings_num = dataframe.shape[0]\n",
        "#Report the number of unique Users and Movies in the dataset\n",
        "print(\"INITIAL DATASET: {0} number of unique users and {1} of unique movies\".format(users_num, movies_num))\n",
        "#Report the total number of existing ratings in the dataset\n",
        "print(\"INITIAL DATASET: {} total number of existing ratings\".format(ratings_num))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64464402-12c9-4080-a87d-6251f0d7beb5",
      "metadata": {
        "id": "64464402-12c9-4080-a87d-6251f0d7beb5"
      },
      "outputs": [],
      "source": [
        "#Define the pickle file that will store the time span per user dataframe\n",
        "pickle_file = os.path.join(DATAFOLDER_PATH, \"ratings_num_df.pkl\")\n",
        "#Check the existence of the previously defined pickle file\n",
        "if os.path.exists(pickle_file):\n",
        "    #Load the pickle file\n",
        "    ratings_num_df = pd.read_pickle(pickle_file)\n",
        "else:\n",
        "    ratings_num_df = dataframe.groupby(\"User\")[\"Rating\"].count().sort_values(ascending=False).reset_index(name=\"ratings_num\")\n",
        "    #Save the previously created dataframe to pickle\n",
        "    ratings_num_df.to_pickle(pickle_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f033c285-a2d3-4daf-9c96-8ebeb47f5103",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "f033c285-a2d3-4daf-9c96-8ebeb47f5103",
        "outputId": "790edc05-0847-49fe-b7d7-6e9b2c543007"
      },
      "outputs": [],
      "source": [
        "#Set the pickle file that will store the time span per user dataframe\n",
        "pickle_file = os.path.join(DATAFOLDER_PATH, \"ratings_span_df.pkl\")\n",
        "if os.path.exists(pickle_file):\n",
        "    ratings_span_df = pd.read_pickle(pickle_file)\n",
        "else:\n",
        "    ratings_span_df = dataframe.groupby(\"User\")[\"Date\"].apply(lambda date: max(date)-min(date)).sort_values(ascending=False).reset_index(name=\"ratings_span\")\n",
        "    ratings_span_df.to_pickle(pickle_file)\n",
        "#Create a new ratings dataframe by joining the previously defined dataframe\n",
        "ratings_df = ratings_num_df.join(ratings_span_df.set_index(\"User\"),on=\"User\")\n",
        "ratings_df[\"ratings_span\"]=ratings_df[\"ratings_span\"].dt.days\n",
        "#Set the threshold values for the minimum and maximum number of Ratings per user\n",
        "minimum_ratings = 100\n",
        "maximum_ratings = 300\n",
        "#Discard all users that do not pertain to the previous range of ratings\n",
        "reduced_ratings_df = ratings_df.loc[(ratings_df[\"ratings_num\"] >= minimum_ratings) & (ratings_df[\"ratings_num\"] <= maximum_ratings)]\n",
        "\n",
        "#Generate the frequency histogram for the number of ratings per user\n",
        "reduced_ratings_df[\"ratings_num\"].plot(kind='hist', title='Frequency of Ratings per User', xticks=range(minimum_ratings, maximum_ratings+1, 25))\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Number of Users')\n",
        "\n",
        "plt.show()\n",
        "#Generate the frequency histogram for the time span of ratings per user\n",
        "reduced_ratings_df[\"ratings_span\"].plot(kind='hist', title='Frequency for time span of Ratings per User')\n",
        "plt.xlabel('Number of Users')\n",
        "plt.ylabel('Time range of Ratings (Days)')\n",
        "\n",
        "plt.show()                                                                                 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49d13f9b-ae75-4625-a9ed-2c1f19bdaa7d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49d13f9b-ae75-4625-a9ed-2c1f19bdaa7d",
        "outputId": "c4ae7618-dc3f-4a97-c5d5-058102d3d8d1"
      },
      "outputs": [],
      "source": [
        "#Get the final dataframe by excluding all users whose ratings fall outside the prespecified range\n",
        "final_df = dataframe.loc[dataframe[\"User\"].isin(reduced_ratings_df[\"User\"])].reset_index()\n",
        "#Drop the links (indices) to the original table\n",
        "final_df = final_df.drop(\"index\", axis=1)\n",
        "#Get the unique users and items in the final dataframe along with the final number of ratings\n",
        "final_users = final_df[\"User\"].unique()\n",
        "final_movies = final_df[\"Movie\"].unique()\n",
        "final_users_num = len(final_users)\n",
        "final_movies_num = len(final_movies)\n",
        "final_ratings_num = len(final_df)\n",
        "\n",
        "#Report the final number of unique users and movies in the dataset\n",
        "print(\"REDUCED DATASET: {0} number of unique users and {1} number of unique movies\".format(final_users_num, final_movies_num))\n",
        "#Report the final number of existing ratings in the dataset\n",
        "print(\"REDUCED DATASET: {} number of existing ratings in the dataset\".format(final_ratings_num))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad131c64-257f-4f1b-9918-e005d7e96910",
      "metadata": {
        "id": "ad131c64-257f-4f1b-9918-e005d7e96910"
      },
      "outputs": [],
      "source": [
        "#We need to reset the users and items IDs in order to be able to construct a network of users and Movies. \n",
        "#Users and Movies IDs should be consecutive in the [1..final_users_num] and [1...final_movies_num]\n",
        "#Initially, we need to acquire the sorted versions of the user and movies\n",
        "sorted_final_users = np.sort(final_users)\n",
        "sorted_final_movies = np.sort(final_movies)\n",
        "#Generate the dictionary of final users as a mapping of the following \n",
        "#sorted_final_users --> [0...final_users_num-1]\n",
        "final_users_dict = dict(zip(sorted_final_users,list(range(0,final_users_num))))\n",
        "#Generate the dictionary of final items as a mapping of the following\n",
        "final_movies_dict = dict(zip(sorted_final_movies,list(range(0,final_movies_num))))\n",
        "#Apply the previously defined dictionary-based maps on the users and movies columns of the final dataframe\n",
        "final_df[\"User\"] = final_df[\"User\"].map(final_users_dict)\n",
        "final_df[\"Movie\"] = final_df[\"Movie\"].map(final_movies_dict)\n",
        "#Get a grouped version of the original dataframe based on the unique final users\n",
        "users_group_df = final_df.groupby(\"User\")\n",
        "#Initialize the adjacency matrix which stores the connection status for pair of users in the recommendation network\n",
        "W = np.zeros((final_users_num, final_users_num))\n",
        "#Iinitialize the matrix storing the number of commonly rated items for a pair of users\n",
        "CommonRatings = np.zeros((final_users_num, final_users_num))\n",
        "#Initialize the matrix of common ratings\n",
        "#Matrix W will be of size [final_users_num x final_users_num],\n",
        "#Let U = {u1, u2,...,un} be the final set of users and I = {i1,i2,...,im}\n",
        "#final set of movies. By considering the function Fi: U -> P(I) where\n",
        "#P(I) is the powerset of I, Fi(u) returns the subset of items that has been rated by user u. \n",
        "#In this context, the edge weight between any given pair of users (u,v) will be computed as:\n",
        "#\n",
        "#          |Intersection(Fi(u)),Fi(v))|\n",
        "#W(u,v) =  -----------------------------\n",
        "#               |Union(Fi(u),Fi(v))|\n",
        "#\n",
        "#\n",
        "#In order to speed up the construction of the adjacency matrix for the ratings network, \n",
        "#construct a dictionary object that will store a set of rated items for each unique user.\n",
        "user_items_dict = {}\n",
        "# for user in final_users:\n",
        "    #print(user)\n",
        "    # user_index = final_users_dict[user]\n",
        "    # user_movies = set(users_group_df.get_group(user_index)[\"Movie\"])\n",
        "    # user_items_dict[user_index] = user_movies\n",
        "                                                 \n",
        "# Initialize the dictionary for storing the set of rated items for each user\n",
        "user_items_dict = {}\n",
        "# print(final_users_dict)\n",
        "# print(sorted_final_users)\n",
        "# print(final_users_dict)\n",
        "# For each unique user, find the set of movies that they rated\n",
        "for user in final_users:\n",
        "    if user in final_users_dict:\n",
        "        user_index = final_users_dict[user]\n",
        "        user_movies = set(users_group_df.get_group(user_index)[\"Movie\"])\n",
        "        user_items_dict[user_index] = user_movies "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0327c71-7ac0-496c-b53c-083ddbbc36de",
      "metadata": {
        "id": "c0327c71-7ac0-496c-b53c-083ddbbc36de"
      },
      "outputs": [],
      "source": [
        "user_ids = list(user_items_dict.keys())\n",
        "user_ids.sort()\n",
        "#Generate the sorted version of the dictionary\n",
        "user_items_dict = {user_index:user_items_dict[user_index] for user_index in user_ids}\n",
        "#Set the pickle file that will store the graph adjacency matrix W.\n",
        "pickle_file_weights = os.path.join(DATAFOLDER_PATH, \"w.npy\")\n",
        "pickle_file_common_ratings = os.path.join(DATAFOLDER_PATH, \"common_ratings.npy\")\n",
        "#Check the existence of the previously defined pickle file\n",
        "if os.path.exists(pickle_file_weights) & os.path.exists(pickle_file_common_ratings):\n",
        "    #Load the pickle file\n",
        "    W = np.load(pickle_file_weights)\n",
        "    CommonRatings = np.load(pickle_file_common_ratings)\n",
        "else:\n",
        "    for source_user in user_items_dict.keys():\n",
        "        for target_user in user_items_dict.keys():\n",
        "            intersection_items = user_items_dict[source_user].intersection(user_items_dict[target_user])\n",
        "            union_items = user_items_dict[source_user].union(user_items_dict[target_user])\n",
        "            W[source_user, target_user] = len(intersection_items)/len(union_items)\n",
        "            CommonRatings[source_user, target_user] = len(intersection_items)\n",
        "    np.save(pickle_file_weights,W)\n",
        "    np.save(pickle_file_common_ratings,CommonRatings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2310ff25-7ad9-4cf6-b6f0-ada4575d6362",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2310ff25-7ad9-4cf6-b6f0-ada4575d6362",
        "outputId": "219ea143-adc4-4c8d-b752-15d0ab8b5565"
      },
      "outputs": [],
      "source": [
        "W"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77097013-5e4f-4e3c-832a-68b3f262e21f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77097013-5e4f-4e3c-832a-68b3f262e21f",
        "outputId": "6a9b4f24-eb38-43ec-ae46-4bd80675cceb"
      },
      "outputs": [],
      "source": [
        "CommonRatings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4959ffd5-63d3-4507-91da-77a2adbdd318",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "4959ffd5-63d3-4507-91da-77a2adbdd318",
        "outputId": "65ebb6a8-f88f-41ba-db04-ac021f0ec780"
      },
      "outputs": [],
      "source": [
        "final_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y7kiJK1IQUYL",
      "metadata": {
        "id": "Y7kiJK1IQUYL"
      },
      "source": [
        "#Δημιουργούμε έναν πίνακα χρηστών - ταινιών \n",
        "(οι χρήστες βρίσκονται στις γραμμές και οι ταινίες στις στήλες του πίνακα)\n",
        "όπου τα στοιχεία του πίνακα είναι από 1 - 10. Εάν ο χρήστης δεν έχει αξιολογήσει την ταινία,\n",
        "η αξιολόγηση που θα ανατεθεί είναι 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "903020b4-0070-41c3-90ef-46f8915f7a3e",
      "metadata": {
        "id": "903020b4-0070-41c3-90ef-46f8915f7a3e"
      },
      "outputs": [],
      "source": [
        "# Create a pivot table of user-movie ratings\n",
        "ratings_matrix_df = final_df.pivot_table(index='User', columns='Movie', values='Rating')\n",
        "ratings_matrix_df = ratings_matrix_df.fillna(0)\n",
        "\n",
        "ratings_matrix_array = ratings_matrix_df.to_numpy()\n",
        "\n",
        "display('ratings_matrix_df', ratings_matrix_df)\n",
        "display('ratings_matrix_array', ratings_matrix_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ceaf6a2-e9e7-4067-b01f-515bc34f59a9",
      "metadata": {
        "id": "3ceaf6a2-e9e7-4067-b01f-515bc34f59a9"
      },
      "outputs": [],
      "source": [
        "from pyclustering.cluster.kmeans import kmeans, kmeans_visualizer\n",
        "from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer\n",
        "from pyclustering.samples.definitions import FCPS_SAMPLES\n",
        "from pyclustering.utils import read_sample\n",
        "from pyclustering.cluster.kmeans import kmeans\n",
        "from pyclustering.utils.metric import type_metric, distance_metric"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KvkvmxZ-QpTc",
      "metadata": {
        "id": "KvkvmxZ-QpTc"
      },
      "source": [
        "Θέλουμε να δημιουργήσουμε τον πίνακα βαρών \"λ\" των χρηστών. Τον πίνακα αξιολογήσεων δηλαδή όπου η τιμή της αξιολόγησης είναι 1 εάν η ταινία έχει αξιολογηθεί από τον χρήστη ή 0 εάν δεν έχει αξιολογηθεί"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a26dcef5-1f13-48b8-9445-74d3ce13a0ce",
      "metadata": {
        "id": "a26dcef5-1f13-48b8-9445-74d3ce13a0ce"
      },
      "outputs": [],
      "source": [
        "# Threshold\n",
        "threshold = 1\n",
        "\n",
        "# Transform to binary\n",
        "binary_matrix = np.where(ratings_matrix_df >= threshold, 1, 0)\n",
        "display('binary_matrix', binary_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f829cfd-4fa3-4ead-820f-2e13e332b22a",
      "metadata": {
        "id": "1f829cfd-4fa3-4ead-820f-2e13e332b22a"
      },
      "outputs": [],
      "source": [
        "# Convert the matrix to a numpy array\n",
        "\n",
        "# Create a dictionary that maps each row of the matrix to its index\n",
        "# matrix_dict = {tuple(row): i for i, row in enumerate(matrix_array)}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QJ0NaNSUrfyC",
      "metadata": {
        "id": "QJ0NaNSUrfyC"
      },
      "source": [
        "# ***Αλγόριθμοι Ομαδοποίησης Δεδομένων***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NJuh-Qpcya-i",
      "metadata": {
        "id": "NJuh-Qpcya-i"
      },
      "source": [
        "**Χρήση της Weighted Euclidean Distance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R8xa7QCu5SG9",
      "metadata": {
        "id": "R8xa7QCu5SG9"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import pdist, cdist\n",
        "import numpy as np\n",
        "\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "def pairwise_weighted_euclidean_distance(X, weights):\n",
        "    # Find the indices of the rated movies for each pair of users\n",
        "    rated_movies = (weights_sparse.T @ weights_sparse) > 0\n",
        "\n",
        "    # Select only the rated movies for each pair of users\n",
        "    X_rated = X_sparse[:, rated_movies]\n",
        "    \n",
        "    # Calculate the pairwise weighted Euclidean distance between \n",
        "    #users who have rated the same movie\n",
        "    return cdist(X, metric='euclidean')\n",
        "\n",
        "def kmeans_pairwise_weighted_euclidean(X, weights, k, max_iters=2):\n",
        "\n",
        "    n, m = X.shape\n",
        "    centroids = X[np.random.choice(n, k, replace=False)]\n",
        "    distances = pairwise_weighted_euclidean_distance(X, weights)\n",
        "    for i in range(max_iters):\n",
        "        # Assign points to clusters\n",
        "        cluster_assignments = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Recalculate cluster centroids\n",
        "        for j in range(k):\n",
        "            cluster_points = X[cluster_assignments == j]\n",
        "            if len(cluster_points) > 0:\n",
        "                centroids[j] = np.average(cluster_points, axis=0)\n",
        "\n",
        "        # Update distances to centroids\n",
        "        distances = pairwise_weighted_euclidean_distance(X, weights)\n",
        "\n",
        "    return cluster_assignments, centroids\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wQx00tek0XXq",
      "metadata": {
        "id": "wQx00tek0XXq"
      },
      "source": [
        "# Clustering users using K-means\n",
        " We want to start by creating the symmetric D matrix which contains the pairwise weighted Euclidean distance for every pair of users.\n",
        " We calculate the distance between each user using \n",
        "*   dist_{u,v}=\\sum_{k=1}^{n}\\sqrt{|R_{u}(k) - R_{v}(k)|λ_{u}(k)λ_{v}(k)}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6911e7b-7bd6-4e50-9197-d57843b540bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6911e7b-7bd6-4e50-9197-d57843b540bb",
        "outputId": "2814987a-7dfd-4a5c-cf24-2ef0b9bfa5c0"
      },
      "outputs": [],
      "source": [
        "# Calculate the pairwise weighted Euclidean distance matrix\n",
        "\n",
        "def create_euclidean_distance_matrix_cached(ratings_matrix: pd.DataFrame, binary_matrix: np_typing.NDArray) -> np_typing.NDArray[np.float64]:\n",
        "    #Set the npy file that will store the Euclidean distance matrix\n",
        "    npy_file = os.path.join(DATAFOLDER_PATH, \"euclidean_distance_matrix.npy\")\n",
        "    if os.path.exists(npy_file):\n",
        "        Dist_euclidean: np_typing.NDArray[np.float64] = np.load(npy_file, allow_pickle=True)\n",
        "        return Dist_euclidean\n",
        "    else:\n",
        "        n = ratings_matrix.shape[0]\n",
        "        Dist_euclidean = np.zeros((n, n))\n",
        "        for i in range(n):\n",
        "            for j in range(i, n):\n",
        "                d = np.sqrt(np.sum(binary_matrix[i,:]*binary_matrix[j,:] * (ratings_matrix.iloc[i,:] - ratings_matrix.iloc[j,:])**2))\n",
        "                Dist_euclidean[i,j] = d\n",
        "                Dist_euclidean[j,i] = d\n",
        "        np.save(npy_file, Dist_euclidean, allow_pickle=True, fix_imports=True)\n",
        "        return Dist_euclidean\n",
        "\n",
        "\n",
        "Dist_euclidean = create_euclidean_distance_matrix_cached(ratings_matrix_df, binary_matrix)\n",
        "Dist_euclidean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "366a9ee9-1b48-46e5-903a-cf7c9f3a7f52",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "366a9ee9-1b48-46e5-903a-cf7c9f3a7f52",
        "outputId": "2595d9ef-3e27-4f58-b6ae-d018de2630ad"
      },
      "outputs": [],
      "source": [
        "df_euclidean = pd.DataFrame(Dist_euclidean)\n",
        "df_euclidean"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NWWDOjPGRCYw",
      "metadata": {
        "id": "NWWDOjPGRCYw"
      },
      "source": [
        "Στον πίνακα αποστάσεων που έχουμε δημιουργήσει, θα τρέξουμε τον αλγόριθμο k-means ώστε να αποτιμήσουμε την ομοιότητα των χρηστών χρησιμοποιώντας τις μεταξύ τους αποστάσεις."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c29300d-deb0-4ebd-b250-aa714a50a0a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c29300d-deb0-4ebd-b250-aa714a50a0a5",
        "outputId": "f38da6e7-1b27-4432-c4ff-e8221596f171"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "# Cluster the users using K-means\n",
        "kmeans = KMeans(n_clusters=L_CLUSTERS_NUM).fit(Dist_euclidean)\n",
        "\n",
        "# Get the cluster labels\n",
        "labels_euclidean = kmeans.labels_\n",
        "\n",
        "# Print the labels\n",
        "print(labels_euclidean)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6hCiZ2wfG2_S",
      "metadata": {
        "id": "6hCiZ2wfG2_S"
      },
      "source": [
        "Cluster the users, by using a custom \n",
        "dist = 1 - np.abs(np.sum(R_u*R_v*weights_u*weights_l)/(np.sqrt(R^2_u*weights_u*weights_l)*np.sqrt(R^2_v*weights_u*weights_l)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a2411e0-9be1-4d00-b1c0-7e58650da9bd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a2411e0-9be1-4d00-b1c0-7e58650da9bd",
        "outputId": "35b4e97d-9a6a-4fa3-f107-bdfc6ddfb562"
      },
      "outputs": [],
      "source": [
        "# Calculate the pairwise weighted Cosine distance matrix\n",
        "\n",
        "def create_cosine_distance_matrix_cached(ratings_matrix: pd.DataFrame, binary_matrix: np_typing.NDArray) -> np_typing.NDArray[np.float64]:\n",
        "    #Set the npy file that will store the Euclidean distance matrix\n",
        "    npy_file = os.path.join(DATAFOLDER_PATH, \"cosine_distance_matrix.npy\")\n",
        "    if os.path.exists(npy_file):\n",
        "        Dist_cosine: np_typing.NDArray[np.float64] = np.load(npy_file, allow_pickle=True)\n",
        "        return Dist_cosine\n",
        "    else:\n",
        "        n = ratings_matrix.shape[0]\n",
        "        Dist_cosine = np.zeros((n, n))\n",
        "        for i in range(n):\n",
        "            for j in range(i, n):\n",
        "                d = 1 - np.abs(np.sum(binary_matrix[i,:] * binary_matrix[j,:] * ratings_matrix.loc[i,:] * ratings_matrix.loc[j,:]) / (np.sqrt(np.sum(binary_matrix[i,:] * binary_matrix[j,:] * ratings_matrix.loc[i,:])* np.sqrt(np.sum(binary_matrix[i,:] * binary_matrix[j,:] * ratings_matrix.loc[j,:])))))\n",
        "                Dist_cosine[i,j] = d\n",
        "                Dist_cosine[j,i] = d\n",
        "        np.save(npy_file, Dist_cosine, allow_pickle=True, fix_imports=True)\n",
        "        return Dist_cosine\n",
        "\n",
        "\n",
        "Dist_cosine = create_cosine_distance_matrix_cached(ratings_matrix_df, binary_matrix)\n",
        "Dist_cosine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cOc0082IqT2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8cOc0082IqT2",
        "outputId": "5220777b-b5db-4d79-b8b7-b8f1259b41df"
      },
      "outputs": [],
      "source": [
        "df_cosine = pd.DataFrame(Dist_cosine)\n",
        "df_cosine = df_cosine.replace(np.nan, 0)\n",
        "df_cosine"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9IvMaCXORgXf",
      "metadata": {
        "id": "9IvMaCXORgXf"
      },
      "source": [
        "Στον πίνακα αποστάσεων που έχουμε δημιουργήσει, θα τρέξουμε τον αλγόριθμο k-means ώστε να αποτιμήσουμε την ομοιότητα των χρηστών χρησιμοποιώντας τις μεταξύ τους αποστάσεις."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1K3Ry-W0IN74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1K3Ry-W0IN74",
        "outputId": "f1fb47bf-4e2e-46a2-a304-40d98233e017"
      },
      "outputs": [],
      "source": [
        "# Cluster the users using K-means\n",
        "kmeans = KMeans(n_clusters=L_CLUSTERS_NUM).fit(df_cosine)\n",
        "\n",
        "# Get the cluster labels\n",
        "labels_cosine = kmeans.labels_\n",
        "\n",
        "# Print the labels\n",
        "print(labels_cosine)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "s_MLiRK6Rh6E",
      "metadata": {
        "id": "s_MLiRK6Rh6E"
      },
      "source": [
        "# Elbow Method\n",
        "Χρησιμοποιούμε την elbow method ώστε να επιλέξουμε τον βέλτιστο αριθμό clusters στον οποίο θα διαχωριστούν τα δεδομένα χρησιμοποιώντας τον k-means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ITnxc3WJec5",
      "metadata": {
        "id": "9ITnxc3WJec5"
      },
      "outputs": [],
      "source": [
        "def elbow_method(df: pd.DataFrame, max_iter: int):\n",
        "  distortions = []\n",
        "  K = range(1,max_iter)\n",
        "  for k in K:\n",
        "    kmeanModel = KMeans(n_clusters=k)\n",
        "    kmeanModel.fit(df)\n",
        "    distortions.append(kmeanModel.inertia_)\n",
        "  plt.figure(figsize=(16,8))\n",
        "  plt.plot(K, distortions, 'bx-')\n",
        "  plt.xlabel('k')\n",
        "  plt.ylabel('Distortion')\n",
        "  plt.title('The Elbow Method showing the optimal k')\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UJgM52L7G1NU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 839
        },
        "id": "UJgM52L7G1NU",
        "outputId": "d498caf8-3470-42a3-df5e-fdd856bef962"
      },
      "outputs": [],
      "source": [
        "#Using the elbow method on Cosine distance\n",
        "elbow_method(df_cosine, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zOCZ4XZyHiKU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 839
        },
        "id": "zOCZ4XZyHiKU",
        "outputId": "897d4299-6e56-449d-c4df-0e29016386d9"
      },
      "outputs": [],
      "source": [
        "#Using the elbow method on Euclidean distance\n",
        "elbow_method(df_euclidean, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "P1K1uNDHW3m4",
      "metadata": {
        "id": "P1K1uNDHW3m4"
      },
      "source": [
        "First, we have to modify our df in order to keep the first n users and assign our labels to them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2oALEUXvXAqJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2oALEUXvXAqJ",
        "outputId": "4a22055e-2afd-49f0-ed60-8a65bfd564e2"
      },
      "outputs": [],
      "source": [
        "# ratings_matrix = ratings_matrix.head(100)\n",
        "ratings_matrix_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AYNf6PMYP-QC",
      "metadata": {
        "id": "AYNf6PMYP-QC"
      },
      "source": [
        "Next, we'll use the PCA method in order to reduce the dimensionality of our matrix and plot our clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ejOUlMYaQOEV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejOUlMYaQOEV",
        "outputId": "fc9ddba9-f3dd-4546-ad48-f206914f041e"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# instantiate StandardScaler and PCA with 2 components for 2D scatter plot\n",
        "scaler = StandardScaler()\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# fit and transform the ratings matrix\n",
        "ratings_pca = pca.fit_transform(ratings_matrix_df)\n",
        "\n",
        "# print the explained variance ratio for each component\n",
        "print(pca.explained_variance_ratio_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UCxdIcygTFor",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UCxdIcygTFor",
        "outputId": "cfa3e3f0-65cb-4438-c872-256480305c0c"
      },
      "outputs": [],
      "source": [
        "# create a new dataframe with the PCA components and user index\n",
        "df_pca = pd.DataFrame(ratings_pca, index=range(0, ratings_matrix_df.shape[0]))\n",
        "df_pca['Cluster'] = labels_euclidean\n",
        "df_pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xt4Rb8BSTKlJ",
      "metadata": {
        "id": "Xt4Rb8BSTKlJ"
      },
      "outputs": [],
      "source": [
        "#Create a function to transform the DF with PCA to 2 coordinates and create a scatter plot\n",
        "\n",
        "def plot_pca_cluster(ratings_matrix, n_clusters):\n",
        "    # instantiate StandardScaler and PCA with 2 components for 2D scatter plot\n",
        "    scaler = StandardScaler()\n",
        "    pca = PCA(n_components=2)\n",
        "\n",
        "    # fit and transform the ratings matrix\n",
        "    ratings_pca = pca.fit_transform(ratings_matrix)\n",
        "\n",
        "    # apply K-means clustering\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    labels = kmeans.fit_predict(ratings_matrix)\n",
        "\n",
        "    # create a new dataframe with the PCA components and cluster labels\n",
        "    df_pca = pd.DataFrame(ratings_pca, index=range(0, ratings_matrix.shape[0]), columns=['Component 1', 'Component 2'])\n",
        "    df_pca['Cluster'] = labels\n",
        "\n",
        "    # create a scatter plot of the PCA components with color-coded clusters\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    for label, color in zip(df_pca['Cluster'].unique(), ['blue', 'red', 'green', 'orange', 'purple']):\n",
        "        group = df_pca.groupby('Cluster').get_group(label)\n",
        "        ax.scatter(group['Component 1'], group['Component 2'], c=color, label=f'Cluster {label}')\n",
        "\n",
        "    # set the axis labels and title\n",
        "    ax.set_xlabel('Component 1')\n",
        "    ax.set_ylabel('Component 2')\n",
        "    ax.set_title('PCA Transformed User-Movie Ratings')\n",
        "\n",
        "    # add a legend\n",
        "    ax.legend()\n",
        "\n",
        "    # show the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2HmCg3SPqa3S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "2HmCg3SPqa3S",
        "outputId": "0ffb00aa-bea7-46eb-fea2-6a0c91ec4bc7"
      },
      "outputs": [],
      "source": [
        "plot_pca_cluster(df_euclidean, L_CLUSTERS_NUM)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yOX_X78d1Wxg",
      "metadata": {
        "id": "yOX_X78d1Wxg"
      },
      "source": [
        "Να σχολιάσετε την αποτελεσματικότητα των συγκεκριμένων μετρικών στην αποτίμηση της ομοιότητας μεταξύ ενός ζεύγους διανυσμάτων προτιμήσεων χρηστών R_u και R_v.\n",
        "\n",
        "Για την μετρική της ευκλείδιας απόστασης: \n",
        "\n",
        "\n",
        "*   Η ομοιότητα των χρηστών είναι **αντιστρόφως ανάλογη** της απόστασης μεταξύ τους.\n",
        "*   Για να έχουμε αποτέλεσμα, θα πρέπει να υπάρχει **επικάλυψη μεταξύ των χρηστών.** Πρέπει δηλαδή να έχουν αξιολογήσει κοινές ταινίες.\n",
        "*   Ο υπολογισμός του k-means γίνεται πολύ πιο υπολογιστικά εντατικός λόγω των εκτεταμένων πολλαπλασιασμών πινάκων που εκτελείται.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Για την μετρική του συνημιτόνου:\n",
        "\n",
        "\n",
        "1.   Για να έχουμε αποτέλεσμα, θα πρέπει να υπάρχει **επικάλυψη μεταξύ των χρηστών.** Πρέπει δηλαδή να έχουν αξιολογήσει κοινές ταινίες.\n",
        "2.   Ο υπολογισμός του k-means γίνεται πολύ πιο υπολογιστικά εντατικός λόγω των εκτεταμένων πολλαπλασιασμών πινάκων που εκτελείται.\n",
        "3.   Η ομοιότητα των χρηστών μπορεί να υπολογιστεί στην περίπτωση που είναι η γωνία μεταξύ των διανυσμάτων τους από 0 - 90 ως ομοιότητα ενώ από 90 - 180 μπορούμε να εκφράσουμε την αντίθεση των χρηστών. Οπότε σε κάθε περίπτωση η μετρική μας βοηθά να ομαδοποιήσουμε τους χρήστες.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qunv_c5V3qgC",
      "metadata": {
        "id": "qunv_c5V3qgC"
      },
      "source": [
        "##JACCARD DISTANCE\n",
        "Η απόσταση Jaccard απομετρά τη **διαφορετικότητα** μεταξύ δύο συνόλων (στην περίπτωσή μας δύο χρηστών). \n",
        "\n",
        "\n",
        "\n",
        "*   Στην περίπτωση που η τομή των δύο χρηστών γίνει μηδέν (δεν υπάρχουν δηλαδή κοινά αξιολογήσιμες ταινίες) η διαφορετικότητα των χρηστών παίρνει τη μέγιστη τιμή της, 1\n",
        "*   Η διαφορετικότητα των χρηστών θα γίνει **ελάχιστη** όταν η *τομή* των δύο χρηστών είναι ίση με την *ένωσή* τους, όταν δηλαδή τα δύο σύνολα γίνουν *ίσα*\n",
        "*   Μπορεί να χρησιμοποιηθεί για τη σύγκριση της ομοιότητας οποιουδήποτε είδους δεδομένων, συμπεριλαμβανομένων δεδομένων χρονοσειρών, φωτογραφιών, κειμένου και εικόνων.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Κάποια από τα μειονεκτήματα της ανωτέρω μετρικής είναι τα ακόλουθα:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   **Απουσία \"βαρών\"**: Η απόσταση Jaccard εξετάζει μόνο την παρουσία ή την απουσία αξιολογήσεων για κάθε χρήστη και δεν λαμβάνει υπόψη τις πραγματικές τιμές αξιολόγησης. Μπορεί δηλαδή η *διαφορετικότητα*, η τιμή δηλαδή που θα προκύψει από την απόσταση Jaccard δύο χρηστών να είναι ελάχιστη, εάν έχουν αξιολογήσει τις ίδιες ταινίες ακόμα και αν ο ένας τις έχει αξιολογήσει με 5 και ο άλλος με 1.\n",
        "*   **Αραιότητα αξιολογήσεων**: Για παράδειγμα, εάν δύο χρήστες έχουν αξιολογήσει μόνο έναν μικρό αριθμό ταινιών, είναι πιθανό να μην έχουν αξιολογήσει καμία από τις ίδιες ταινίες, άρα η τομή τους θα είναι μηδέν, με αποτέλεσμα η *διαφορετικότητά* τους να είναι μέγιστη, ακόμη και αν οι προτιμήσεις τους για τις ταινίες είναι στην πραγματικότητα αρκετά παρόμοιες. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E2udjYdk7qUs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2udjYdk7qUs",
        "outputId": "213800f7-ab68-4bba-9646-ec5d7dab9bac"
      },
      "outputs": [],
      "source": [
        "jaccard_dist = 1 - W\n",
        "jaccard_df = pd.DataFrame(jaccard_dist)\n",
        "\n",
        "def kmeans_clustering(jaccard_dist, L):\n",
        "    # Initialize k-means object\n",
        "    kmeans = KMeans(n_clusters=L)\n",
        "\n",
        "    # Fit the k-means object to the Jaccard distance matrix\n",
        "    kmeans.fit(jaccard_dist)\n",
        "\n",
        "    return kmeans.labels_\n",
        "\n",
        "def create_jaccard_labels_cached(jaccard_dist, L: int):\n",
        "    npy_file = os.path.join(DATAFOLDER_PATH, \"jaccard_labels.npy\")\n",
        "    if os.path.exists(npy_file):\n",
        "        jaccard_labels: np_typing.NDArray = np.load(npy_file, allow_pickle=True)\n",
        "        return jaccard_labels\n",
        "    else:\n",
        "        jaccard_labels = kmeans_clustering(jaccard_dist, L)\n",
        "        np.save(npy_file, jaccard_labels, allow_pickle=True, fix_imports=True)\n",
        "        return jaccard_labels\n",
        "\n",
        "\n",
        "jaccard_labels = create_jaccard_labels_cached(jaccard_dist, L_CLUSTERS_NUM)\n",
        "display('jaccard_df', jaccard_df)\n",
        "display('jaccard_dist', jaccard_dist)\n",
        "display('jaccard_labels', jaccard_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_fHEwfARTLtN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "_fHEwfARTLtN",
        "outputId": "ad6534a6-fc38-4bc1-f4bd-4139f02082c6"
      },
      "outputs": [],
      "source": [
        "plot_pca_cluster(jaccard_dist, L_CLUSTERS_NUM)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o9_hstoB_hPW",
      "metadata": {
        "id": "o9_hstoB_hPW"
      },
      "source": [
        "# Neural Network \n",
        "\n",
        "### Pre - processing \n",
        "\n",
        "We will first start by seperating our Users according to the Cluster they've been assigned to, using the Jaccard distance on the K-Means algorithm.\n",
        "\n",
        "We do this by creating a df containing the ratings of each user and the Cluster it belongs to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CBq0dgSGAOWf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CBq0dgSGAOWf",
        "outputId": "d72524c5-1ecb-4685-95f4-ece41dc4a180"
      },
      "outputs": [],
      "source": [
        "ratings_matrix_clustered = ratings_matrix_df\n",
        "\n",
        "ratings_matrix_clustered['Cluster'] = jaccard_labels\n",
        "\n",
        "ratings_matrix_clustered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Dr4FO-I6GAoY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dr4FO-I6GAoY",
        "outputId": "016539d2-2456-40de-951a-37eea38029eb"
      },
      "outputs": [],
      "source": [
        "#We sort the Labels of the Clusters from 0 to 4\n",
        "# clusters = sorted(ratings_matrix_clustered.Cluster.unique())\n",
        "# clusters = ratings_matrix_clustered.Cluster.unique()\n",
        "\n",
        "# #We save each Cluster in an array where each position is for the same Cluster\n",
        "# clustered_DFs: list[np_typing.NDArray] = []\n",
        "# for cluster in clusters:\n",
        "#     groupby_result = ratings_matrix_clustered.groupby('Cluster').get_group(cluster)\n",
        "#     clustered_DFs.append(groupby_result.to_numpy())\n",
        "# display('clustered_DFs', clustered_DFs)\n",
        "\n",
        "\n",
        "\n",
        "# For each cluster find the ratings and the jaccard distances\n",
        "# Each cluster_ratings array has shape (#cluster users, #total movies)\n",
        "# Each cluster_jaccard_distances array has shape (#cluster users, #total users)\n",
        "# Each cluster_users_indexes array has a set with len(#cluster users)\n",
        "\n",
        "def calculate_clusters_ratings_jaccard_distances_users_indexes(L_CLUSTERS_NUM: int, jaccard_labels: np_typing.NDArray, ratings_matrix_array: np_typing.NDArray, jaccard_dist: np_typing.NDArray):\n",
        "    clusters_ratings_dict = { i: [] for i in range(L_CLUSTERS_NUM)}\n",
        "    clusters_jaccard_distances_dict = { i: [] for i in range(L_CLUSTERS_NUM)}\n",
        "    clusters_users_indexes_dict: dict[int, set[int]] = { i: set() for i in range(L_CLUSTERS_NUM)}\n",
        "\n",
        "    for i in range(jaccard_labels.shape[0]):\n",
        "        label = jaccard_labels[i]\n",
        "        cluster_ratings = clusters_ratings_dict[label]\n",
        "        cluster_jaccard_distances = clusters_jaccard_distances_dict[label]\n",
        "        cluster_users_indexes = clusters_users_indexes_dict[label]\n",
        "\n",
        "        cluster_ratings.append(ratings_matrix_array[i])\n",
        "        cluster_jaccard_distances.append(jaccard_dist[i])\n",
        "        cluster_users_indexes.add(i)\n",
        "\n",
        "\n",
        "    clusters_ratings_list: list[np_typing.NDArray] = []\n",
        "    for key in clusters_ratings_dict:\n",
        "        cluster_ratings = np.array(clusters_ratings_dict[key])\n",
        "        clusters_ratings_list.append(cluster_ratings)\n",
        "\n",
        "\n",
        "    clusters_jaccard_distances_list: list[np_typing.NDArray] = []\n",
        "    for key in clusters_jaccard_distances_dict:\n",
        "        cluster_jaccard_distances = np.array(clusters_jaccard_distances_dict[key])\n",
        "        clusters_jaccard_distances_list.append(cluster_jaccard_distances)\n",
        "    \n",
        "    clusters_users_indexes_list: list[set[int]] = []\n",
        "    for key in clusters_users_indexes_dict:\n",
        "        clusters_users_indexes_list.append(clusters_users_indexes_dict[key])\n",
        "\n",
        "\n",
        "    # clusters_ratings cotains the cluster_ratings array for each cluster\n",
        "    clusters_ratings = np.array(clusters_ratings_list)\n",
        "    # clusters_jaccard_distances cotains the cluster_jaccard_distances array for each cluster\n",
        "    cluster_jaccard_distances = np.array(clusters_jaccard_distances_list)\n",
        "    # clusters_users_indexes cotains the users indexes that belong in this cluster\n",
        "    clusters_users_indexes = np.array(clusters_users_indexes_list)\n",
        "    return clusters_ratings, cluster_jaccard_distances, clusters_users_indexes\n",
        "\n",
        "\n",
        "clusters_ratings, clusters_jaccard_distances, clusters_users_indexes = calculate_clusters_ratings_jaccard_distances_users_indexes(L_CLUSTERS_NUM, jaccard_labels, ratings_matrix_array, jaccard_dist)\n",
        "display(f'clusters_ratings[0].shape: {clusters_ratings[0].shape}')\n",
        "display(f'clusters_jaccard_distances[0].shape: {clusters_jaccard_distances[0].shape}')\n",
        "display(f'len(clusters_users_indexes[0]): {len(clusters_users_indexes[0])}')\n",
        "display('clusters_ratings', clusters_ratings)\n",
        "display('clusters_jaccard_distances', clusters_jaccard_distances)\n",
        "display('clusters_users_indexes', clusters_users_indexes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q25Gys3fYJt7",
      "metadata": {
        "id": "Q25Gys3fYJt7"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# define the custom distance metric based on Jaccard similarity score\n",
        "def custom_distance(u, v):\n",
        "    intersection = len(set(u).intersection(set(v)))\n",
        "    union = len(set(u).union(set(v)))\n",
        "    similarity = intersection / union\n",
        "    return 1 - similarity\n",
        "\n",
        "\n",
        "def find_nearest_neighbors(cluster_ratings_binary: np_typing.NDArray, k: int):\n",
        "    # instantiate the NearestNeighbors model with the custom distance metric\n",
        "    model = NearestNeighbors(n_neighbors=k, algorithm='auto', metric=custom_distance)\n",
        "\n",
        "    # fit the model on the ratings matrix\n",
        "    model.fit(cluster_ratings_binary)\n",
        "\n",
        "    # find the k-nearest neighbors for each user\n",
        "    k_nearest_neighbors: list[list[int]] = []\n",
        "    \n",
        "    distances, indices = model.kneighbors(cluster_ratings_binary, n_neighbors=k+1) # get indices of k+1 most similar users (including the user itself)\n",
        "    for i in range(indices.shape[0]):\n",
        "        # exclude the user itself\n",
        "        neighbors = [index for index in indices[i] if index != i]\n",
        "        k_nearest_neighbors.append(neighbors[:k])\n",
        "\n",
        "\n",
        "    display('cluster_ratings_binary.shape', cluster_ratings_binary.shape)\n",
        "    display('indices.shape', indices.shape)\n",
        "    display('indices', indices)\n",
        "\n",
        "    # for i in range(cluster_ratings.shape[0]):\n",
        "    #     # _, indices = model.kneighbors([cluster_ratings[i]], n_neighbors=k+1) # get indices of k+1 most similar users (including the user itself)\n",
        "\n",
        "    #     # if we want to get the distance for each pair of users\n",
        "    #     # neighbors = [(index, custom_distance(ratings[i], ratings[index])) for index in indices[0] if index != i] \n",
        "    #     neighbors = [index for index in indices[0] if index != i]\n",
        "    #     # exclude the user itself\n",
        "    #     k_nearest_neighbors.append(neighbors[:k])\n",
        "\n",
        "    # We save our k_nearest_neighbors as a dict where for each user, we get the \n",
        "    # most similar of their users. This will allow us to \n",
        "    # Create a NN where the INPUT: will be the ratings of similar users\n",
        "    # and OUTPUT: the rating of the user we currently have.\n",
        "    return np.array(k_nearest_neighbors)\n",
        "\n",
        "\n",
        "# def calculate_jaccard_value(source_ratings: np_typing.NDArray, target_ratings: np_typing.NDArray):\n",
        "#     n = source_ratings.shape[0]\n",
        "#     intersection_len = 0\n",
        "#     union_len = 0\n",
        "#     for i in range(n):\n",
        "#         if source_ratings[i] != 0.0 and target_ratings[i] != 0.0:\n",
        "#             intersection_len += 1\n",
        "        \n",
        "\n",
        "#         if source_ratings[i] != 0.0 or target_ratings[i] != 0.0:\n",
        "#             union_len += 1\n",
        "\n",
        "\n",
        "#     return 1 - intersection_len / union_len\n",
        "\n",
        "\n",
        "# def create_cluster_jaccard(cluster_ratings: np_typing.NDArray):\n",
        "#     cluster_users_num = cluster_ratings.shape[0]\n",
        "#     jaccard_lists: list[list[float]] = []\n",
        "#     for source_user in range(cluster_users_num):\n",
        "#         jaccard_list: list[float] = []\n",
        "#         for target_user in range(cluster_users_num):\n",
        "#             jaccard_value = calculate_jaccard_value(cluster_ratings[source_user], cluster_ratings[target_user])\n",
        "#             jaccard_list.append(jaccard_value)\n",
        "        \n",
        "\n",
        "#         jaccard_lists.append(jaccard_list)\n",
        "\n",
        "\n",
        "#     return np.array(jaccard_lists)\n",
        "\n",
        "\n",
        "# def find_nearest_neighbors_using_jaccard(cluster_ratings: np_typing.NDArray, k: int):\n",
        "#     cluster_jaccard = create_cluster_jaccard(cluster_ratings)\n",
        "#     display('cluster_jaccard', cluster_jaccard)\n",
        "\n",
        "#     nearest_neighbors_list: list[np_typing.NDArray] = []\n",
        "#     for row_index in range(cluster_jaccard.shape[0]):\n",
        "#         cluster_jaccard_row = cluster_jaccard[row_index]\n",
        "#         k_nearest_indexes = np.argpartition(cluster_jaccard_row, k)\n",
        "#         k_nearest_indexes = k_nearest_indexes[k_nearest_indexes != row_index]\n",
        "#         nearest_neighbors_list.append(k_nearest_indexes[:k])\n",
        "        \n",
        "    \n",
        "#     return np.array(nearest_neighbors_list)\n",
        "\n",
        "def find_nearest_neighbors_using_jaccard(K_NEIGHBORS_NUM: int, cluster_jaccard_distances: np_typing.NDArray, cluster_users_indexes: set[int]):\n",
        "    nearest_neighbors_list: list[np_typing.NDArray] = []\n",
        "    for row_index in range(cluster_jaccard_distances.shape[0]):\n",
        "        cluster_jaccard_row = cluster_jaccard_distances[row_index]\n",
        "        \n",
        "        ## set the distance of the users that are the same a index or don't belong in the cluster to a value higher thatn 1\n",
        "        for j in range(cluster_jaccard_row.shape[0]):\n",
        "            if (j == row_index or j in cluster_users_indexes):\n",
        "                cluster_jaccard_row[j] = 2\n",
        "\n",
        "        ## find the k smallest indexes\n",
        "        k_nearest_indexes = np.argpartition(cluster_jaccard_row, K_NEIGHBORS_NUM+1)\n",
        "        k_nearest_indexes = k_nearest_indexes[k_nearest_indexes != row_index]\n",
        "        nearest_neighbors_list.append(k_nearest_indexes[:K_NEIGHBORS_NUM])        \n",
        "    \n",
        "    return np.array(nearest_neighbors_list)\n",
        "\n",
        "\n",
        "def create_clusters_nearest_neighbors_cached(K_NEIGHBORS_NUM: int, clusters_jaccard_distances: np_typing.NDArray, clusters_users_indexes: np_typing.NDArray):\n",
        "    #Set the npy file that will store the clusters_nearest_neighbors\n",
        "    npy_file = os.path.join(DATAFOLDER_PATH, \"clusters_nearest_neighbors.npy\")\n",
        "    if os.path.exists(npy_file):\n",
        "        clusters_nearest_neighbors: np_typing.NDArray[np.float64] = np.load(npy_file, allow_pickle=True)\n",
        "        return clusters_nearest_neighbors\n",
        "    else:\n",
        "        clusters_nearest_neighbors_list: list[np_typing.NDArray] = []        \n",
        "        for index in range(clusters_jaccard_distances.shape[0]):\n",
        "            cluster_jaccard_distances = clusters_jaccard_distances[index]\n",
        "            # cluster_ratings_binary = np.where(cluster_ratings > 0, 1, 0)\n",
        "            # nearest_neihbors = find_nearest_neighbors(cluster_ratings_binary, k)\n",
        "            nearest_neihbors = find_nearest_neighbors_using_jaccard(K_NEIGHBORS_NUM, clusters_jaccard_distances[index].copy(), clusters_users_indexes[index])\n",
        "            display('nearest_neihbors', nearest_neihbors)\n",
        "            clusters_nearest_neighbors_list.append(nearest_neihbors)            \n",
        "        \n",
        "        \n",
        "        clusters_nearest_neighbors = np.array(clusters_nearest_neighbors_list)\n",
        "        np.save(npy_file, clusters_nearest_neighbors, allow_pickle=True, fix_imports=True)\n",
        "        return clusters_nearest_neighbors\n",
        "    \n",
        "\n",
        "clusters_nearest_neighbors = create_clusters_nearest_neighbors_cached(K_NEIGHBORS_NUM, clusters_jaccard_distances, clusters_users_indexes)\n",
        "display('clusters_nearest_neighbors', clusters_nearest_neighbors)\n",
        "\n",
        "# # instantiate the NearestNeighbors model with the custom distance metric\n",
        "# model = NearestNeighbors(n_neighbors=k, algorithm='brute', metric=custom_distance)\n",
        "\n",
        "# cluster_ratings = clustered_DFs[1]\n",
        "# # fit the model on the ratings matrix\n",
        "# model.fit(cluster_ratings)\n",
        "\n",
        "# # find the k-nearest neighbors for each user\n",
        "# k_nearest_neighbors = {}\n",
        "# for i in range(cluster_ratings.shape[0]):\n",
        "#     _, indices = model.kneighbors([cluster_ratings[i]], n_neighbors=k+1) # get indices of k+1 most similar users (including the user itself)\n",
        "#     # if we want to get the distance for each pair of users\n",
        "#     # neighbors = [(index, custom_distance(ratings[i], ratings[index])) for index in indices[0] if index != i] \n",
        "#     neighbors = [index for index in indices[0] if index != i]\n",
        "#  # exclude the user itself\n",
        "#     k_nearest_neighbors[i] = neighbors\n",
        "\n",
        "# # We save our k_nearest_neighbors as a dict where for each user, we get the \n",
        "# # most similar of their users. This will allow us to \n",
        "# # Create a NN where the INPUT: will be the ratings of similar users\n",
        "# # and OUTPUT: the rating of the user we currently have.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6OIxYAAvDEOH",
      "metadata": {
        "id": "6OIxYAAvDEOH"
      },
      "source": [
        "#Creating the NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Rsktpb_zDDK7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rsktpb_zDDK7",
        "outputId": "b1432f3b-1edb-403c-eb77-64db8be7cc6d"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# collect the ratings of the similar users and the target user\n",
        "# similar_user_ratings = []\n",
        "# target_user_ratings = []\n",
        "# cluster_ratings = cluster_ratings / 5\n",
        "# for i, neighbors in k_nearest_neighbors.items():\n",
        "#     similar_ratings = cluster_ratings[neighbors]\n",
        "#     target_ratings = cluster_ratings[i]\n",
        "#     similar_user_ratings.append(similar_ratings)\n",
        "#     target_user_ratings.append(target_ratings)\n",
        "\n",
        "# # convert the lists of ratings to numpy arrays\n",
        "# similar_user_ratings = np.array(similar_user_ratings)\n",
        "# target_user_ratings = np.array(target_user_ratings)\n",
        "\n",
        "# create a neural network model\n",
        "# model = tf.keras.Sequential([\n",
        "#     tf.keras.layers.Dense(256, activation='relu', input_shape=(k, 5043)),\n",
        "#     tf.keras.layers.Dense(128, activation='relu'),\n",
        "#     tf.keras.layers.Dense(5043)\n",
        "# ])\n",
        "\n",
        "# # compile the model with an appropriate optimizer and loss function\n",
        "# model.compile(optimizer='adam', loss='mse')\n",
        "# # train the model using the nearest neighbors' ratings as input and the user's rating as output\n",
        "# for user_id in range(ratings.shape[0]):\n",
        "#     neighbors_ratings = [ratings[neighbor] for neighbor in k_nearest_neighbors[user_id] if neighbor != user_id]\n",
        "#     user_rating = ratings[user_id]\n",
        "#     X = np.array([neighbors_ratings])\n",
        "#     print(X.shape)\n",
        "#     y = np.array([user_rating])\n",
        "#     model.fit(X, y, epochs=10, batch_size=32)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5b42d0b2",
      "metadata": {},
      "source": [
        "# THANOS NEURAL NETWORK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "019b5102",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define functions to create and train a linear regression model\n",
        "def create_nn_original_df(ratings_matrix_array: np_typing.NDArray, cluster_ratings: np_typing.NDArray, nearest_neighbors: np_typing.NDArray, NEIGHBOURS_COLUMNS: list[str]):\n",
        "    # create user_ratings_list\n",
        "    user_index_list: list[int] = []\n",
        "    user_ratings_list: list[int] = []\n",
        "    for i in range(nearest_neighbors.shape[0]):\n",
        "        user_index_list.extend([i for ratings in cluster_ratings[i]])\n",
        "        user_ratings_list.extend(cluster_ratings[i])\n",
        "        \n",
        "    # create neighbors list\n",
        "    neighbors_list: list[list[int]] = []\n",
        "    for i in range(nearest_neighbors.shape[1]):\n",
        "        neighbor_ratings_list: list[int] = []\n",
        "\n",
        "        for j in range(nearest_neighbors.shape[0]):\n",
        "            neighbor = nearest_neighbors[j][i]\n",
        "            # nearest neighbors have indexs for the ratings_matrix_array up to total users\n",
        "            # so use ratings_matrix_array instead of cluster_ratings.\n",
        "            # We have previously ensured that all neighbors belong to this clusters\n",
        "            neighbor_ratings_list.extend(ratings_matrix_array[neighbor])\n",
        "\n",
        "        neighbors_list.append(neighbor_ratings_list)\n",
        "\n",
        "\n",
        "    nn_origin_df = pd.DataFrame()\n",
        "    nn_origin_df['USER_INDEX'] = user_index_list\n",
        "    nn_origin_df['USER_RATINGS'] = user_ratings_list\n",
        "\n",
        "    for i in range(len(neighbors_list)):\n",
        "        neighbor_ratings_list: list[int] = neighbors_list[i]\n",
        "        nn_origin_df[NEIGHBOURS_COLUMNS[i]] = neighbor_ratings_list\n",
        "\n",
        "\n",
        "    return nn_origin_df\n",
        "\n",
        "\n",
        "def create_nn_filtered_normalized_df(ratings_matrix_array: np_typing.NDArray, cluster_ratings: np_typing.NDArray, ratings_normalize_factor, nearest_neighbors, NEIGHBOURS_COLUMNS: list[str]):\n",
        "    nn_origin_df = create_nn_original_df(ratings_matrix_array, cluster_ratings, nearest_neighbors, NEIGHBOURS_COLUMNS)\n",
        "\n",
        "    # create filtered dataframe\n",
        "    nn_filtered_df = nn_origin_df.copy()[nn_origin_df['USER_RATINGS'] != 0]\n",
        "\n",
        "    # create filtered normalized df\n",
        "    nn_filtered_normalized_df = nn_filtered_df.copy()\n",
        "    columns_to_normalize = ['USER_RATINGS']\n",
        "    columns_to_normalize.extend(NEIGHBOURS_COLUMNS)\n",
        "    nn_filtered_normalized_df[columns_to_normalize] = nn_filtered_normalized_df[columns_to_normalize] / ratings_normalize_factor\n",
        "    display('Dataframe with filter user ratings (non zero) and neighbors ratings scaled by maximum rating')\n",
        "    display(nn_filtered_normalized_df)\n",
        "    display(nn_filtered_normalized_df.describe())\n",
        "    return nn_origin_df, nn_filtered_df, nn_filtered_normalized_df\n",
        "\n",
        "\n",
        "def create_feature_columns(NEIGHBOURS_COLUMNS: list[str]):\n",
        "    \"\"\"Create feature columns\"\"\"\n",
        "    feature_columns = []\n",
        "    for column in NEIGHBOURS_COLUMNS:\n",
        "        feature_columns.append(tf.feature_column.numeric_column(column))\n",
        "\n",
        "    return feature_columns\n",
        "\n",
        "\n",
        "def create_model(my_learning_rate, feature_columns):\n",
        "    \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
        "    # Most simple tf.keras models are sequential.\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    # model.add(tf.keras.layers.Masking(\n",
        "    #     mask_value=0.0, input_shape=(1,)\n",
        "    # ))\n",
        "    \n",
        "    # Add the layer containing the feature columns to the model.\n",
        "    model.add(tf.keras.layers.DenseFeatures(feature_columns))\n",
        "\n",
        "    # Implement L2 regularization in the first hidden layer.\n",
        "    model.add(tf.keras.layers.Dense(units=20, \n",
        "                                    activation='relu',\n",
        "                                    kernel_regularizer=tf.keras.regularizers.l2(0.04),\n",
        "                                    name='Hidden1'))\n",
        "    \n",
        "    # Implement L2 regularization in the second hidden layer.\n",
        "    model.add(tf.keras.layers.Dense(units=12, \n",
        "                                    activation='relu', \n",
        "                                    kernel_regularizer=tf.keras.regularizers.l2(0.04),\n",
        "                                    name='Hidden2'))\n",
        "\n",
        "    # Define the output layer.\n",
        "    model.add(tf.keras.layers.Dense(units=1,  \n",
        "                                    name='Output'))                              \n",
        "    \n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=my_learning_rate),\n",
        "                    loss=\"mean_squared_error\",\n",
        "                    metrics=[tf.keras.metrics.MeanSquaredError(), tf.keras.metrics.MeanAbsoluteError()])\n",
        "    return model           \n",
        "\n",
        "\n",
        "def train_model(model: tf.keras.models.Sequential, train_df: pd.DataFrame, NEIGHBOURS_COLUMNS: list[str], epochs: int, batch_size: int=1):\n",
        "    \"\"\"Train the model by feeding it data.\"\"\"\n",
        "\n",
        "    # Features as a dictionary with key the neighbor ratings column name and value an ndarray of the values\n",
        "    features = {\n",
        "        column: np.array(train_df[column]) for column in NEIGHBOURS_COLUMNS\n",
        "    }\n",
        "    label = np.array(train_df['USER_RATINGS'])\n",
        "    history = model.fit(x=features, y=label, batch_size=batch_size, epochs=epochs, shuffle=True) \n",
        "\n",
        "    # The list of epochs is stored separately from the rest of history.\n",
        "    epochs = history.epoch\n",
        "    \n",
        "    # To track the progression of training, gather a snapshot\n",
        "    # of the model's mean squared error at each epoch. \n",
        "    hist = pd.DataFrame(history.history)\n",
        "    mse = hist[\"mean_squared_error\"]\n",
        "    mae = hist[\"mean_absolute_error\"]\n",
        "\n",
        "    return epochs, mse, mae\n",
        "\n",
        "\n",
        "def evaluate_model(model: tf.keras.models.Sequential, test_df: pd.DataFrame, NEIGHBOURS_COLUMNS: list[str], batch_size: int=1):\n",
        "    \"\"\"Evaluate the model with the test_df\"\"\"\n",
        "\n",
        "    # Features as a dictionary with key the neighbor ratings column name and value an ndarray of the values\n",
        "    features = {\n",
        "        column: np.array(test_df[column]) for column in NEIGHBOURS_COLUMNS\n",
        "    }\n",
        "    label = np.array(test_df['USER_RATINGS'])\n",
        "    return model.evaluate(x = features, y = label, batch_size=batch_size)\n",
        "\n",
        "\n",
        "def predict_model(model: tf.keras.models.Sequential, nn_origin_df: pd.DataFrame, NEIGHBOURS_COLUMNS: list[str], batch_size: int=1):\n",
        "    \"\"\"Evaluate the model with the test_df\"\"\"\n",
        "\n",
        "    # Features as a dictionary with key the neighbor ratings column name and value an ndarray of the values\n",
        "    features = {\n",
        "        column: np.array(nn_origin_df[column]) for column in NEIGHBOURS_COLUMNS\n",
        "    }\n",
        "    return model.predict(x = features, batch_size=batch_size)\n",
        "\n",
        "\n",
        "def plot_the_loss_curve(epochs, mse_or_mae, is_mse: bool):\n",
        "    \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
        "\n",
        "    plt.figure()\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    ylabel = 'Train Mean Squared Error' if is_mse else 'Train Mean Absolute Error'\n",
        "    plt.ylabel(ylabel)\n",
        "\n",
        "    plt.plot(epochs, mse_or_mae, label=\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.ylim([mse_or_mae.min()*0.95, mse_or_mae.max() * 1.03])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def calculate_real_mse_mae(origin_ratings: np_typing.NDArray, predictions: np_typing.NDArray):\n",
        "    \"\"\"Calculate the real mse and mae comparing real ratings and predictions.\"\"\"\n",
        "\n",
        "    n = 0\n",
        "    absolute_sum = 0\n",
        "    squared_sum = 0\n",
        "    for i in range(origin_ratings.shape[0]):\n",
        "        # take into consideration only non 0 ratings\n",
        "        if origin_ratings[i] != 0.0:\n",
        "            n += 1\n",
        "            abs_value = abs(origin_ratings[i] - predictions[i])\n",
        "            absolute_sum += abs_value\n",
        "            squared_sum += math.sqrt(abs_value)\n",
        "\n",
        "    \n",
        "    mse = absolute_sum / n\n",
        "    mae = squared_sum / n\n",
        "    return mse, mae\n",
        "\n",
        "\n",
        "def create_train_evaluate_neural_network(ratings_normalize_factor, nn_origin_df: pd.DataFrame, nn_filtered_normalized_df: pd.DataFrame, NEIGHBOURS_COLUMNS: list[str]) -> tuple[float, float, float, float, float, float]:\n",
        "    train_df, test_df = train_test_split(nn_filtered_normalized_df, test_size=0.2, random_state=42)\n",
        "    train_df = pd.DataFrame(train_df)\n",
        "    test_df = pd.DataFrame(test_df)\n",
        "\n",
        "    display('train_df', train_df)\n",
        "    display('test_df', test_df)\n",
        "\n",
        "    # The following variables are the hyperparameters.\n",
        "    learning_rate = 0.01\n",
        "    epochs = 64\n",
        "    batch_size = 96\n",
        "\n",
        "    # define the feature columns\n",
        "    feature_columns = create_feature_columns(NEIGHBOURS_COLUMNS)\n",
        "    # Establish the model's topography.\n",
        "    my_model = create_model(learning_rate, feature_columns)\n",
        "\n",
        "    # Train the model on the normalized training set.\n",
        "    display('Training the model with the train_df')\n",
        "    epochs, train_mse_series, train_mae_series = train_model(my_model, train_df, NEIGHBOURS_COLUMNS, epochs, batch_size)\n",
        "    train_mse = train_mse_series.iloc[-1]\n",
        "    train_mae = train_mae_series.iloc[-1]\n",
        "    plot_the_loss_curve(epochs, train_mse_series, True)\n",
        "    plot_the_loss_curve(epochs, train_mae_series, False)\n",
        "\n",
        "\n",
        "    display('Evaluating the model against the test_df')\n",
        "    test_loss, test_mse, test_mae = evaluate_model(my_model, test_df, NEIGHBOURS_COLUMNS, batch_size)\n",
        "\n",
        "    predictions_normalized = predict_model(my_model, nn_origin_df, NEIGHBOURS_COLUMNS, batch_size)\n",
        "    # turn list of sinle item lists to a sigle list with floats\n",
        "    predictions_normalized = np.array([prediction_normalized[0] for prediction_normalized in predictions_normalized])\n",
        "    predictions = predictions_normalized * ratings_normalize_factor\n",
        "    display('predictions', predictions)\n",
        "    display('predictions.shape', predictions.shape)\n",
        "\n",
        "    real_ratings = nn_origin_df['USER_RATINGS'].to_numpy()\n",
        "    display('real_ratings', real_ratings)\n",
        "    real_mse, real_mae = calculate_real_mse_mae(real_ratings, predictions_normalized)\n",
        "    display(f'real_mean_squared_error={real_mse}, real_mean_absolute_error={real_mae}')\n",
        "\n",
        "    return train_mse, train_mae, test_mse, test_mae, real_mse, real_mae\n",
        "\n",
        "\n",
        "# THANOS NEURAL NETWORK\n",
        "ratings_normalize_factor: float = ratings_matrix_df.max().max()\n",
        "# NOTE: ratings_normalize_factor == 1 (not normalizing data) produces better results from normalizing data\n",
        "ratings_normalize_factor = 1\n",
        "display('ratings_normalize_factor', ratings_normalize_factor)\n",
        "\n",
        "# Create, train and evaluate a Neural Network for each cluster\n",
        "\n",
        "NEIGHBOURS_COLUMNS = [f'NEIGHBOR_RATINGS_{i}' for i in range(K_NEIGHBORS_NUM)]\n",
        "results: list[list[float]] = []\n",
        "results_df_index: list[str] = []\n",
        "\n",
        "for cluster_index in range(len(clusters_ratings)):\n",
        "    cluster_ratings = clusters_ratings[cluster_index]\n",
        "    nearest_neighbors = clusters_nearest_neighbors[cluster_index]    \n",
        "\n",
        "    nn_origin_df, nn_filtered_df, nn_filtered_normalized_df = create_nn_filtered_normalized_df(ratings_matrix_array, cluster_ratings, \n",
        "        ratings_normalize_factor, nearest_neighbors, NEIGHBOURS_COLUMNS)\n",
        "\n",
        "    train_mse, train_mae, test_mse, test_mae, real_mse, real_mae = create_train_evaluate_neural_network(\n",
        "        ratings_normalize_factor, nn_origin_df, nn_filtered_normalized_df, NEIGHBOURS_COLUMNS)\n",
        "    results.append([train_mse, train_mae, test_mse,\n",
        "                   test_mae, real_mse, real_mae])\n",
        "    results_df_index.append(f'CLUSTER_{cluster_index}')\n",
        "\n",
        "\n",
        "results_df_columns = ['TRAIN_MSE', 'TRAIN_MAE',\n",
        "                      'TEST_MSE', 'TEST_MAE', 'REAL_MSE', 'REAL_MAE']\n",
        "results_df = pd.DataFrame(\n",
        "    results, columns=results_df_columns, index=results_df_index)\n",
        "display('results_df', results_df)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "toc-autonumbering": true,
    "toc-showcode": true,
    "toc-showmarkdowntxt": false,
    "toc-showtags": false
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
